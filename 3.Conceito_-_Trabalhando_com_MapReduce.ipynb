{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe735a8f",
   "metadata": {},
   "source": [
    "# <center>Labs Trabalhando com o MapReduce</center>\n",
    "\n",
    "### IMPORTANTE\n",
    "\n",
    "- Para estes laboratórios foram fornecidos os arquivos python como `AvaliaFilme.py` , `AnalisaFilme.py` e `AmigosIdade.py` entre outros contendo **Jobs de MapReduce** utilizando a linguagem Python e também os arquivos `ml-100k.zip` contendo o **dataset** a ser usado no *Laboratório 1* assim como outros datasets como `amigos_facebook.csv` entre outros.\n",
    "\n",
    "<br> <b>\n",
    "    \n",
    "---\n",
    "    \n",
    "    \n",
    "<br> <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9f947",
   "metadata": {},
   "source": [
    "# O que é MapReduce?\n",
    "\n",
    "O MapReduce é um modelo de programação distribuído utilizado principalmente em frameworks como o Hadoop para processar grandes volumes de dados. Ele permite que problemas complexos sejam divididos em pequenas partes que podem ser processadas paralelamente em um cluster de servidores. Esse modelo é particularmente útil para cenários de Big Data, onde o volume de dados é grande demais para ser processado de maneira eficiente em uma única máquina.\n",
    "\n",
    "### Como Funciona o Modelo MapReduce\n",
    "\n",
    "O modelo MapReduce é composto por duas fases principais: **Map** (Mapeamento) e **Reduce** (Redução), além de uma fase intermediária chamada **Shuffle**.\n",
    "\n",
    "- **1. Fase de Mapeamento (Map)**: O processo começa com o cientista de dados analisando o problema e definindo como os dados de entrada serão representados em pares de chave e valor. Por exemplo, em uma contagem de palavras em um conjunto de textos, as palavras individuais podem ser as chaves e a contagem (geralmente 1) seria o valor associado.\n",
    "\n",
    "O cientista de dados desenvolve um programa de mapeamento (Mapper), que aplica essas regras de negócios, \"quebrando\" os dados de entrada em pequenos pares de chave/valor. Cada bloco de dados de entrada é então processado pelo Mapper, que gera os pares conforme definido no código.\n",
    "\n",
    "**Exemplo**: Se os dados de entrada forem \"Deer, Bear, River, Car\", o Mapper pode gerar os seguintes pares: (\"Deer\", 1), (\"Bear\", 1), (\"River\", 1), (\"Car\", 1).\n",
    "\n",
    "<br>\n",
    "\n",
    "- **2. Fase Intermediária** - Shuffle: Após a fase de mapeamento, ocorre a fase de Shuffle. O Shuffle é executado automaticamente pelo framework (neste caso, o Hadoop), sem que o cientista de dados precise intervir diretamente. A função dessa fase é agrupar todos os pares de chave/valor gerados pela etapa de mapeamento.\n",
    "\n",
    "**Exemplo**: Se em várias partes do texto foram encontradas várias ocorrências da palavra \"Car\", o Shuffle agrupará todas essas ocorrências em um único lugar para que possam ser processadas na fase seguinte.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **3. Fase de Redução (Reduce)**: Na fase de Reduce, os pares agrupados pelo Shuffle são processados para gerar o resultado final. O cientista de dados define como será a redução dos dados. No caso da contagem de palavras, a redução pode ser simplesmente somar os valores para cada chave.\n",
    "\n",
    "**Exemplo**: Se os pares de chave/valor após o Shuffle são (\"Car\", [1, 1, 1]), a fase de redução somará esses valores, resultando em (\"Car\", 3).\n",
    "\n",
    "Essa fase de redução retorna a informação processada e consolidada que o cientista de dados precisa para resolver o problema em questão. No exemplo, a saída seria uma lista das palavras e suas respectivas contagens, como (\"Car\", 3), (\"Deer\", 2), (\"Bear\", 2).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3adfb",
   "metadata": {},
   "source": [
    "# <center>Laboratório 1 (Avalia Filme)</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "# Objetivo do Laboratório 1\n",
    "\n",
    "<br>\n",
    "\n",
    "Utilizando um **dataset de sistema de recomendação de filmes**, o objetivo é **executar um programa** que conte a quantidade de avaliações recebidas para cada tipo de nota atribuída aos filmes.\n",
    "\n",
    "Em outras palavras, vamos verificar **quantas vezes cada filme recebeu determinadas avaliações**.\n",
    "\n",
    "Por exemplo, se um filme recebeu a nota <i>**4 estrelas**</i> uma vez ou <i>**5 estrelas**</i> sete vezes, isso será contabilizado e apresentado de forma estruturada.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Pergunta de Negócio\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Quantas avaliações de cada tipo um filme recebeu?**\n",
    "\n",
    "Esta questão nos ajudará a entender o comportamento das avaliações, identificando quais tipos de notas são mais comuns e qual é a distribuição das avaliações para diferentes filmes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82452e5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Sobre o Dataset\n",
    "\n",
    "<br>\n",
    "\n",
    "Para este laboratório usaremos o dataset **MovieLens 100K** que é um dos conjuntos de dados mais utilizados para sistemas de recomendação e foi criado pelo GroupLens Research. Ele é frequentemente empregado para avaliar o desempenho de algoritmos colaborativos e análise preditiva na recomendação de filmes.\n",
    "\n",
    "---\n",
    "\n",
    "### Descrição Geral do Dataset MovieLens 100K\n",
    "\n",
    "- **Lançamento**: Abril de 1998\n",
    "- **Tamanho**: 100.000 avaliações\n",
    "- **Número de usuários**: 1.000\n",
    "  - Cada usuário avaliou pelo menos 20 filmes.\n",
    "- **Número de filmes**: 1.700\n",
    "  - Filmes lançados até 1998, com múltiplos gêneros.\n",
    "- **Formato das avaliações**: Escala de 1 a 5 estrelas.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Estrutura dos Arquivos\n",
    "\n",
    "- **`u.data`** (**será usado neste laboratório**):\n",
    "  - Contém as avaliações no formato (usuário, filme, nota, timestamp).\n",
    "  - **Exemplo**: 196 242 3 881250949 (Usuário 196 deu nota 3 para o filme 242 no timestamp correspondente).\n",
    "\n",
    "- **`u.user`**: \n",
    "  - Informações sobre os usuários, como (ID, idade, sexo, ocupação, CEP).\n",
    "  - **Exemplo**: 1|24|M|technician|85711.\n",
    "\n",
    "- **`u.item`**:\n",
    "  - Detalhes dos filmes, como (ID, título, data de lançamento, gêneros).\n",
    "  - **Exemplo**: 1|Toy Story (1995)|01-Jan-1995|Animation|Children's|Comedy.\n",
    "\n",
    "- **`u.genre`**:\n",
    "  - Lista os gêneros disponíveis, como Action, Comedy, Drama, etc.\n",
    "\n",
    "- **`u.occupation`**:\n",
    "  - Lista as ocupações dos usuários.\n",
    "  \n",
    "<br>\n",
    "\n",
    "- **Arquivos de Treino e Teste** (`*.base` e `*.test`):\n",
    "\n",
    " - O dataset é dividido em conjuntos para **treinamento** e **teste**, usados para validar algoritmos de recomendação.\n",
    "  \n",
    "<br>\n",
    "\n",
    "- **README.txt**:\n",
    "  - Contém instruções sobre a utilização do dataset, incluindo os termos de licença.\n",
    "\n",
    "---\n",
    "\n",
    "### Aplicações e Casos de Uso\n",
    "\n",
    "- **Teste de Sistemas de Recomendação**: Avaliação de algoritmos colaborativos, como Filtragem Colaborativa e Modelos de Fatoração de Matrizes.\n",
    "- **Análise de Comportamento do Usuário**: Identificação de padrões nas avaliações e preferências.\n",
    "- **Treinamento e Validação de Modelos de Machine Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac517c",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# <center><u>Iniciando o Laboratório 1</u></center>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "## 1. Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "1.1 **Iniciar o HDFS (NameNode, DataNode, SecondaryNameNode)**:\n",
    "   ```bash\n",
    "   start-dfs.sh  |  stop-dfs.sh\n",
    "   ```\n",
    "1.2 **Iniciar o YARN (ResourceManager, NodeManager)**:\n",
    "   ```bash\n",
    "   start-yarn.sh  |  stop-yarn.sh\n",
    "   ```\n",
    "1.3 **Verificando serviços**:\n",
    "   ```bash\n",
    "   jps\n",
    "   ```\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 2. Criando Pasta/Diretório com o nome de `mapred` para o Laboratório no HDFS\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    hdfs dfs -mkdir /mapred\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "2.1 **Lista os arquivos e diretórios no HDFS raiz**.\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "   hdfs dfs -ls /\n",
    "```\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 3. Copiando o Dataset do `Sistema Operacional Local` para dentro do `HDFS`:\n",
    "\n",
    "<br>\n",
    "\n",
    "**Para este laboratório precisaremos do arquivo `u.data` que está dentro da pasta extraída do dataset**.\n",
    "\n",
    "Abrir pasta onde está o arquivo a ser copiado, abrir um terminal e digitar o comando abaixo:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    hdfs dfs -put u.data /mapred\n",
    "```\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 4. Criando Arquivo Python com o Job para o MapReduce\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `AvaliaFilme.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRAvaliaFilme(MRJob):\n",
    "    def mapper(self, key, line):\n",
    "        # Dividimos cada linha por tabulação e extraímos os valores\n",
    "        (userID, movieID, rating, timestamp) = line.split('\\t')\n",
    "        # Emitimos a nota como chave e 1 como valor\n",
    "        yield rating, 1\n",
    "\n",
    "    def reducer(self, rating, occurences):\n",
    "        # Somamos todas as ocorrências da mesma avaliação\n",
    "        yield rating, sum(occurences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Executa o job MapReduce\n",
    "    MRAvaliaFilme.run()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.1 Entendendo as 3 Fases do Processo de <i>MapReduce</i> considerando o Script `AvaliaFilme.py`\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### Fase 1 – Mapeamento (Mapper)\n",
    "\n",
    "![Example Image](Analytics/image1.png)\n",
    "\n",
    "Na fase de mapeamento, o objetivo é **contar quantas vezes cada filme foi avaliado com uma determinada nota**. Utilizamos a palavra reservada `yield` para definir a **chave** que será usada no processo. No nosso caso, a **coluna** `rating` é escolhida como chave, pois queremos saber o **número total de filmes avaliados para cada tipo de nota**, variando de **1 a 5 estrelas**.\n",
    "\n",
    "Durante esta fase, cada **rating** é **mapeado** e associado ao valor **1**, que representa uma única ocorrência dessa nota para um filme. Este passo é crucial para contabilizar a frequência de cada avaliação.\n",
    "\n",
    "O **código** responsável por essa fase é escrito pelo **Cientista de Dados**, que define a lógica de como processar as linhas de entrada para gerar os pares de **chave-valor**.\n",
    "\n",
    "#### Explicando o trecho do código:\n",
    "\n",
    "- A função `mapper` recebe como entrada cada linha do arquivo de dados.\n",
    "- A linha é dividida em **quatro partes**: `userID`, `movieID`, `rating` e `timestamp`, com os valores separados por tabulações (`\\t`).\n",
    "- A chave escolhida é a **nota (rating)** dada ao filme, enquanto o valor é o número **1**. Isso indica que para cada avaliação feita, estamos registrando **uma ocorrência** da nota.\n",
    "- O `yield` é a palavra-chave que emite cada par **chave-valor** para a próxima fase do processo.\n",
    "\n",
    "<br>\n",
    "\n",
    "Esse processo de mapeamento é fundamental para preparar os dados para a próxima fase, onde as avaliações serão agrupadas e somadas.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### Fase 2 – Shuffle e Sort\n",
    "\n",
    "Essa fase é **processada automaticamente pelo framework MapReduce**. Não há um código explícito para essa fase no script, pois o próprio framework cuida de:\n",
    "\n",
    "- Agrupar todos os pares de **chave-valor** emitidos pela função `mapper`.\n",
    "- Organizar os dados com base nas **chaves iguais** (neste caso, os ratings de 1 a 5).\n",
    "- Após o agrupamento, os pares chave-valor para as notas são semelhantes ao seguinte exemplo:\n",
    "\n",
    "**Exemplo de Agrupamento:**\n",
    "\n",
    "```makefile\n",
    "    1: [1, 1]\n",
    "    2: [1, 1, 1]\n",
    "    3: [1, 1]\n",
    "    4: [1]\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### Fase 3 – Redução (Reducer)\n",
    "\n",
    "![Example Image](Analytics/image2.png)\n",
    "\n",
    "Na fase de **redução**, o código aplica um cálculo matemático para somar as ocorrências de cada nota (rating). Utilizamos a função `sum()` para agregar todas as ocorrências de uma nota específica, resultando na **quantidade total de filmes avaliados** com aquela nota.\n",
    "\n",
    "Essa fase é fundamental, pois transforma os dados mapeados em informações úteis: quantos filmes receberam cada tipo de avaliação.\n",
    "\n",
    "O **Cientista de Dados** é responsável por definir como essa redução será aplicada, usando a lógica apropriada.\n",
    "\n",
    "#### Explicando o trecho do código:\n",
    "\n",
    "- A função `reducer` recebe como entrada a **nota (rating)** como chave, e uma lista de **ocorrências** (que são os números 1 gerados na fase de mapeamento).\n",
    "- A função `sum()` é aplicada a essa lista, somando todas as ocorrências de cada nota.\n",
    "- O `yield` emite o **rating (chave)** e o total de ocorrências (quantidade de filmes que receberam aquela nota).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Exemplo de Resultado Final**:\n",
    "\n",
    "```makefile\n",
    "    1: 2\n",
    "    2: 3\n",
    "    3: 2\n",
    "    4: 1\n",
    "```\n",
    "\n",
    "**Isso indica que**: 2 filmes receberam nota 1, 3 filmes receberam nota 2, 2 filmes receberam nota 3 e 1 filme recebeu nota 4\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "### Resumo das 3 Fases com Código Explicado\n",
    "\n",
    "- **Mapper (Fase 1)**: Divide as linhas do arquivo e emite pares de chave-valor, onde a **chave é a nota** (`rating`) e o **valor é 1** para cada ocorrência da nota.\n",
    "- **Shuffle e Sort (Fase 2)**: O framework organiza automaticamente as notas iguais e agrupa as ocorrências.\n",
    "- **Reducer (Fase 3)**: Soma as ocorrências de cada nota, retornando o total de filmes para cada tipo de nota.\n",
    "\n",
    "<br>\n",
    "\n",
    "Esse processo permite **contabilizar o número de filmes** que receberam cada tipo de avaliação (de 1 a 5 estrelas) no dataset.\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 5. Executando Arquivo Python com o Job para o MapReduce\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Como Executar um Job MapReduce ?\n",
    "\n",
    "<br>\n",
    "\n",
    "> Para executar um **Job MapReduce**, precisamos rodar um script Python que utiliza o framework Hadoop para processar os dados. Aqui está uma **explicação sobre como realizar essa execução**.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "#### 5.1 Executando o Script Python:\n",
    "\n",
    "<br>\n",
    "\n",
    "Normalmente, para executar um script Python, bastaria abrir um terminal no diretório onde o arquivo se encontra e digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    python AvaliaFilme.py\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### 5.2 Passando o Conjunto de Dados como Entrada:\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, o arquivo Python `AvaliaFilme.py` **precisa receber como entrada um conjunto de dados que está armazenado no HDFS (sistema de arquivos distribuído do Hadoop)**.\n",
    "\n",
    "Para isso, precisamos indicar corretamente o caminho do arquivo de dados no **HDFS** durante a execução do script, da seguinte forma:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    python AvaliaFilme.py hdfs:///mapred/u.data -r hadoop\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- `hdfs:///mapred/u.data` é o caminho do arquivo de dados no HDFS.\n",
    "- `-r hadoop` indica que o job será executado usando o Hadoop como backend.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### 5.3 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Job Completado com Sucesso**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    Job job_1728940457090_0001 completed successfully\n",
    "```\n",
    "<br>\n",
    "\n",
    "<i>**Map e Reduce Concluídos**</i>: O processo de MapReduce envolve duas etapas: **mapeamento (map)** e **redução (reduce)**. O status de 100% para ambas as fases confirma que o processamento foi concluído corretamente:\n",
    "\n",
    "```bash\n",
    "    map 100% reduce 100%\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado. No nosso exemplo, vemos a quantidade de filmes que receberam cada tipo de nota:\n",
    "\n",
    "```bash\n",
    "    \"1\"  6110\n",
    "    \"2\"  11370\n",
    "    \"3\"  27145\n",
    "    \"4\"  34174\n",
    "    \"5\"  21201\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: 6.110 filmes receberam nota 1, 11.370 filmes receberam nota 2, 27.145 filmes receberam nota 3, 34.174 filmes receberam nota 4 e 21.201 filmes receberam nota 5.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Limpeza dos Diretórios Temporários**</i>: Após a conclusão do job, os diretórios temporários no **HDFS** são removidos automaticamente para manter o sistema organizado. Isso garante que nenhum arquivo desnecessário permaneça no sistema após o processamento:\n",
    "\n",
    "```bash\n",
    "    Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/AvaliaFilme.hadoop.20241016.205700.452291...\n",
    "```\n",
    "\n",
    "<br> <br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> <br> <br> <br> <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e54299",
   "metadata": {},
   "source": [
    "# <center>Laboratório 2 (Gerar Média de Amigos)</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "# Objetivo do Laboratório 2\n",
    "\n",
    "<br>\n",
    "\n",
    "Utilizando um **dataset que contém dados sobre o número de amigos no Facebook e idades dos usuários**, o objetivo é **executar um programa MapReduce** para calcular a **média de amigos no Facebook por idade**.\n",
    "\n",
    "Em outras palavras, vamos **calcular a média do número de amigos para cada faixa etária**. Isso nos permitirá entender melhor o comportamento de amizade no Facebook, observando qual faixa etária possui mais conexões na plataforma.\n",
    "\n",
    "**Por exemplo**, se usuários de 30 anos têm, em média, **200 amigos** e usuários de 40 anos têm, em média, **450 amigos**, esses resultados serão computados e apresentados.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Pergunta de Negócio\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Qual é a média de amigos no Facebook por faixa etária?**\n",
    "\n",
    "Esta questão nos ajudará a entender o comportamento social dos usuários do Facebook, verificando qual faixa etária tende a ter mais ou menos amigos.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Sobre o Dataset\n",
    "\n",
    "<br>\n",
    "\n",
    "Para este laboratório, vamos utilizar um **dataset de usuários do Facebook**, que contém informações sobre **nomes**, **idades**, e o **número de amigos** que cada usuário possui. O objetivo será calcular a **média de amigos no Facebook por faixa etária**, analisando como o número de amigos varia conforme a idade dos usuários.\n",
    "\n",
    "#### Descrição Geral do Novo Dataset\n",
    "\n",
    "- **Número de registros (usuários)**: 500\n",
    "- **Colunas**: 4\n",
    "  - **ID do usuário**: Número de identificação único de cada usuário.\n",
    "  - **Nome**: Nome do usuário.\n",
    "  - **Idade**: Idade do usuário.\n",
    "  - **Número de Amigos**: Quantidade de amigos que o usuário possui no Facebook.\n",
    "  \n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# <center><u>Iniciando o Laboratório 2</u></center>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "## 1. Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "1.1 **Iniciar o HDFS (NameNode, DataNode, SecondaryNameNode)**:\n",
    "   ```bash\n",
    "   start-dfs.sh  |  stop-dfs.sh\n",
    "   ```\n",
    "1.2 **Iniciar o YARN (ResourceManager, NodeManager)**:\n",
    "   ```bash\n",
    "   start-yarn.sh  |  stop-yarn.sh\n",
    "   ```\n",
    "1.3 **Verificando serviços**:\n",
    "   ```bash\n",
    "   jps\n",
    "   ```\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 2. Criando Pasta/Diretório com o nome de `media_facebook` para o Laboratório no HDFS\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    hdfs dfs -mkdir /media_facebook\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "2.1 **Lista os arquivos e diretórios no HDFS raiz**.\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "   hdfs dfs -ls /\n",
    "```\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 3. Copiando o Dataset do `Sistema Operacional Local` para dentro do `HDFS`:\n",
    "\n",
    "<br>\n",
    "\n",
    "**Para este laboratório precisaremos do arquivo `amigos_facebook.csv`**.\n",
    "\n",
    "Abrir pasta onde está o arquivo a ser copiado, abrir um terminal e digitar o comando abaixo:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    hdfs dfs -put amigos_facebook.csv /media_facebook\n",
    "```\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 4. Criando Arquivo Python com o Job para o MapReduce\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `AmigosIdade.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRAmigosPorIdade(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        (ID, nome, idade, numAmigos) = line.split(',')\n",
    "        yield idade, float(numAmigos)\n",
    "\n",
    "    def reducer(self, idade, numAmigos):\n",
    "        total = 0\n",
    "        numElementos = 0\n",
    "        for x in numAmigos:\n",
    "            total += x\n",
    "            numElementos += 1\n",
    "            \n",
    "        yield idade, total / numElementos\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRAmigosPorIdade.run()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.1 Explicando Código do Job MapReduce (`AmigosIdade.py`)\n",
    "\n",
    "<br>\n",
    "\n",
    "O código **AmigosIdade.py** implementa um **job MapReduce** para calcular a média de amigos no Facebook por faixa etária. Ele é composto de duas partes principais: **mapper** e **reducer**. Aqui está uma explicação detalhada de cada componente:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Mapper:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def mapper(self, _, line):\n",
    "    (ID, nome, idade, numAmigos) = line.split(',')\n",
    "    yield idade, float(numAmigos)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `mapper` recebe cada linha do dataset, que contém informações separadas por vírgula (ID, nome, idade e número de amigos).\n",
    "\n",
    "A função então:\n",
    "\n",
    "- **Divide a linha** em suas respectivas colunas: `ID`, `nome`, `idade` e `numAmigos`.\n",
    "- **Emite a idade como chave** e o número de amigos como valor para o próximo estágio do processo (reduce).\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Reducer:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def reducer(self, idade, numAmigos):\n",
    "    total = 0\n",
    "    numElementos = 0\n",
    "    for x in numAmigos:\n",
    "        total += x\n",
    "        numElementos += 1\n",
    "        \n",
    "    yield idade, total / numElementos\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `reducer` recebe todas as entradas agrupadas pela mesma chave (idade), ou seja, para cada faixa etária, ela obtém uma lista de valores (número de amigos). A função então:\n",
    "\n",
    "- **Soma todos os valores** (número de amigos) e conta o número de elementos.\n",
    "- **Calcula a média** dividindo o total pelo número de entradas.\n",
    "- **Emite a idade** e a média de amigos como o resultado final.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.2 Resumo\n",
    "\n",
    "- O **mapper** separa os dados por linha e emite a idade como chave e o número de amigos como valor.\n",
    "- O **reducer** calcula a média de amigos por idade, somando os valores e dividindo pelo número de elementos.\n",
    "- O resultado final é a **média de amigos para cada idade** no dataset.\n",
    "\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 5. Aplicando o job MapReduce\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, usaremos o arquivo Python `AmigosIdade.py` para aplicar o **job MapReduce**. Para isso basta ir ao diretório do arquivo AmigosIdade.py e no terminal digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "    python AmigosIdade.py hdfs:///media_facebook/amigos_facebook.csv -r hadoop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.1 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Job Completed Successfully**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado. No nosso exemplo, vemos a quantidade de filmes que receberam cada tipo de nota:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "\"18\"\t343.375\n",
    "\"19\"\t213.27272727272728\n",
    "\"20\"\t165.0\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: Usuários de 18 anos têm, em média, 343.375 amigos no Facebook, usuários de 19 anos têm, em média, 213.27 amigos no Facebook, usuários de 20 anos têm, em média, 165 amigos no Facebook.\n",
    "\n",
    "E assim por diante para cada idade, conforme o resultado exibido. O job calculou com sucesso a média de amigos por idade usando o algoritmo MapReduce.\n",
    "\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br><br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c40d0d",
   "metadata": {},
   "source": [
    "# <center>Laboratório 3 (Data Mining com MapReduce em Dados Não Estruturados)</center>\n",
    "\n",
    "\n",
    "### O que é Data Mining ?\n",
    "\n",
    "**Data Mining (ou mineração de dados)** é o processo de explorar grandes conjuntos de dados para descobrir padrões ocultos, tendências e informações úteis. Ele envolve o uso de técnicas de análise de dados, como estatísticas, aprendizado de máquina e visualização de dados, para extrair conhecimento e tomar decisões baseadas em dados.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# Objetivo do Laboratório 3\n",
    "\n",
    "<br>\n",
    "\n",
    "Realizar **Data Mining** em um **dataset de texto não estruturado**, especificamente o livro **Orgulho e Preconceito** de **Jane Austen**. Usaremos **MapReduce** para realizar **três diferentes jobs** de análise no texto, cada um com suas regras, buscando explorar e extrair informações a partir desse material.\n",
    "\n",
    "O **objetivo** deste laboratório é entender como aplicar técnicas de mineração de dados em dados não estruturados (texto) e explorar informações relevantes, como a frequência de palavras, identificação de palavras únicas e análise de frases.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Perguntas de Negócio\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Quais são as palavras mais frequentes no livro \"Orgulho e Preconceito\" de Jane Austen e quantas vezes cada uma aparece?**\n",
    "\n",
    "> **Quantas vezes cada palavra aparece no texto, considerando regras de tokenização mais precisas?**\n",
    "\n",
    "> **Quais são as palavras mais frequentes no texto, organizadas em ordem decrescente de frequência?**\n",
    "\n",
    "<br>\n",
    "\n",
    "Estas questões\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Sobre o Dataset\n",
    "\n",
    "<br>\n",
    "\n",
    "Para este laboratório, utilizaremos o livro **Orgulho e Preconceito** de **Jane Austen**. O dataset é um arquivo de texto simples que contém o conteúdo completo do livro. O objetivo é realizar mineração de dados sobre o conteúdo textual utilizando jobs de MapReduce.\n",
    "  \n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# <center><u>Iniciando o Laboratório 3</u></center>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "## 1. Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "1.1 **Iniciar o HDFS (NameNode, DataNode, SecondaryNameNode)**:\n",
    "   ```bash\n",
    "   start-dfs.sh  |  stop-dfs.sh\n",
    "   ```\n",
    "1.2 **Iniciar o YARN (ResourceManager, NodeManager)**:\n",
    "   ```bash\n",
    "   start-yarn.sh  |  stop-yarn.sh\n",
    "   ```\n",
    "1.3 **Verificando serviços**:\n",
    "   ```bash\n",
    "   jps\n",
    "   ```\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 2. Criando Pasta/Diretório com o nome de `data_mining` para o Laboratório no HDFS\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    hdfs dfs -mkdir /data_mining\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "2.1 **Lista os arquivos e diretórios no HDFS raiz**.\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "   hdfs dfs -ls /\n",
    "```\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 3. Copiando o Dataset do `Sistema Operacional Local` para dentro do `HDFS`:\n",
    "\n",
    "<br>\n",
    "\n",
    "**Para este laboratório precisaremos do arquivo `OrgulhoePreconceito.txt`**.\n",
    "\n",
    "Abrir pasta onde está o arquivo a ser copiado, abrir um terminal e digitar o comando abaixo:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    hdfs dfs -put OrgulhoePreconceito.txt /data_mining\n",
    "```\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> <br> <br> <br> \n",
    "\n",
    "## 4. Criando Arquivo Python com o Job para o MapReduce 1 (`MR-DataMining-1.py`)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `MR-DataMining-1.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class MRDataMining(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        palavras = line.split()\n",
    "        for palavra in palavras:\n",
    "            yield palavra.lower(), 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRDataMining.run()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.1 Explicando Código do Job MapReduce 1\n",
    "\n",
    "<br>\n",
    "\n",
    "O código **MR-DataMining-1.py** implementa um **job MapReduce** para contar a frequência de cada palavra em um texto\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Mapper:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def mapper(self, _, line):\n",
    "        palavras = line.split()\n",
    "        for palavra in palavras:\n",
    "            yield palavra.lower(), 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `mapper`:\n",
    "\n",
    "- A função `mapper` lê o arquivo de entrada linha por linha.\n",
    "- Cada linha é dividida em palavras usando o método `split()`.\n",
    "- Em seguida, o código transforma cada palavra em minúsculas (`lower()`) para garantir que \"Palavra\" e \"palavra\" sejam tratadas como a mesma palavra.\n",
    "- Para cada palavra encontrada, o `mapper` emite um par chave-valor onde a **palavra é a chave e o número **1** é o valor, indicando que a palavra apareceu uma vez.\n",
    "\n",
    "- \n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Reducer:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `reducer`:\n",
    "\n",
    "- A função `reducer` recebe todas as ocorrências da mesma palavra (agrupadas pela chave, que é a palavra).\n",
    "- Em seguida, a função soma todos os valores (o número de ocorrências da palavra) e emite a palavra junto com a soma total.\n",
    "\n",
    "- \n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.2 Resumo\n",
    "\n",
    "- Este **job MapReduce** lê um texto, divide-o em palavras, conta quantas vezes cada palavra aparece, e retorna a contagem total para cada palavra. O uso de `lower()` garante que as contagens não diferenciem maiúsculas de minúsculas, tratando \"Palavra\" e \"palavra\" como a mesma.\n",
    "\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 5. Aplicando o job MapReduce 1\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, usaremos o arquivo Python `MR-DataMining-1.py` para aplicar o **job MapReduce**. Para isso basta ir ao diretório do arquivo e no terminal digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "    python MR-DataMining-1.py hdfs:///data_mining/OrgulhoePreconceito.txt -r hadoop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.1 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Job Completed Successfully**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado.\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "\"your\"\t426\n",
    "\"yours\"\t2\n",
    "\"yours,\"\t4\n",
    "\"yours,\\\"\"\t2\n",
    "\"yours.\"\t3\n",
    "\"yours.\\\"\"\t3\n",
    "\"yourself\"\t28\n",
    "\"yourself,\"\t9\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: Nosso job MapReduce conseguiu detectar a quantidade de palavras de forma bem simples, **isso signfica** que ele **não consegue** diferencias `yours` de `yours,` que possuí `,` por exemplo. Precisaremos cuidar disso. É o que faremos no passo 6.\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 6. Criando Arquivo Python com o Job para o MapReduce 2 (`MR-DataMining-2.py`)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `MR-DataMining-2.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "REGEXP_PALAVRA = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRDataMining(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        palavras = REGEXP_PALAVRA.findall(line)\n",
    "        for palavra in palavras:\n",
    "            yield palavra.lower(), 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRDataMining.run()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6.1 Explicando Código do Job MapReduce 2\n",
    "\n",
    "<br>\n",
    "\n",
    "O código **MR-DataMining-2.py** é uma versão aprimorada do primeiro job, onde aplicamos uma **tokenização mais precisa** (filtrando `pontuações` ao lado de palavras) para identificar as palavras de forma correta no texto, removendo pontuações como vírgulas e pontos finais.\n",
    "<br>\n",
    "\n",
    "#### 1. Mapper:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def mapper(self, _, line):\n",
    "        palavras = REGEXP_PALAVRA.findall(line)\n",
    "        for palavra in palavras:\n",
    "            yield palavra.lower(), 1\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `mapper`:\n",
    "\n",
    "- A função `mapper` lê o arquivo de entrada linha por linha, mas, em vez de usar `split()` como no job anterior, aqui usamos uma **expressão regular** (definida pela variável `REGEXP_PALAVRA`) para extrair apenas palavras, ignorando pontuações, como vírgulas, pontos e outros caracteres especiais.\n",
    "- Cada palavra extraída é convertida para minúscula com `lower()` e emitida com o valor `1`, indicando que a palavra apareceu uma vez.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Reducer:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `reducer`:\n",
    "\n",
    "- A função `reducer` recebe todas as ocorrências de cada palavra (agrupadas pela chave, que é a palavra) e soma os valores para determinar o número total de vezes que a palavra apareceu no texto.\n",
    "- Emite a palavra e sua respectiva contagem total.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6.2 Resumo\n",
    "\n",
    "- Este **job MapReduce** melhora a tokenização ao usar expressões regulares para **limpar o texto**, removendo pontuações e outros caracteres especiais.\n",
    "- O `mapper` divide o texto em palavras de forma mais precisa, enquanto o `reducer` conta o número total de vezes que cada palavra aparece.\n",
    "- Esta abordagem corrige o problema observado no **Job 1**, onde palavras como `\"yours\"` e `\"yours,\"` eram tratadas como diferentes devido à pontuaçã\n",
    "\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 7. Aplicando o job MapReduce 2\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, usaremos o arquivo Python `MR-DataMining-2.py` para aplicar o **job MapReduce**. Para isso basta ir ao diretório do arquivo e no terminal digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "    python MR-DataMining-2.py hdfs:///data_mining/OrgulhoePreconceito.txt -r hadoop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.1 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Job Completed Successfully**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado.\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "\"youngest\"\t13\n",
    "\"your\"\t448\n",
    "\"yours\"\t21\n",
    "\"yourself\"\t50\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: Palavras e a quantidade de vezes que foram usadas no livro.\n",
    "\n",
    "\n",
    "<br><br><br><br> \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 8. Criando Arquivo Python com o Job para o MapReduce 3 (`MR-DataMining-3.py`)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `MR-DataMining-3.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "REGEXP_PALAVRA = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRDataMining(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper = self.mapper_get_words, reducer = self.reducer_count_words),\n",
    "            MRStep(mapper = self.mapper_make_counts_key, reducer = self.reducer_output_words)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        palavras = REGEXP_PALAVRA.findall(line)\n",
    "        for palavra in palavras:\n",
    "            yield palavra.lower(), 1\n",
    "\n",
    "    def reducer_count_words(self, palavra, values):\n",
    "        yield palavra, sum(values)\n",
    "\n",
    "    def mapper_make_counts_key(self, palavra, count):\n",
    "        yield '%04d'%int(count), palavra\n",
    "\n",
    "    def reducer_output_words(self, count, palavras):\n",
    "        for palavra in palavras:\n",
    "            yield count, palavra\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRDataMining.run()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 8.1 Explicando Código do Job MapReduce 3\n",
    "\n",
    "<br>\n",
    "\n",
    "O código **MR-DataMining-3.py** implementa um **job MapReduce** *mais complexo*, que envolve dois passos (steps). Primeiro, ele conta o número de ocorrências de cada palavra, e, em seguida, organiza essas palavras em **ordem decrescente de frequência**. Este job utiliza **duas fases de mapeamento e redução**.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 0. Steps: \n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def steps(self):\n",
    "    return [\n",
    "        MRStep(mapper=self.mapper_get_words, reducer=self.reducer_count_words),\n",
    "        MRStep(mapper=self.mapper_make_counts_key, reducer=self.reducer_output_words)\n",
    "    ]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "- A função `steps` no código **MR-DataMining-3.py** é usada para **definir múltiplos passos (ou fases) no job de MapReduce**. Ela organiza a sequência de mapeamento e redução que o job irá executar, permitindo que o código tenha mais de uma fase de **MapReduce**.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Mapper1: `mapper_get_words`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def mapper_get_words(self, _, line):\n",
    "        palavras = REGEXP_PALAVRA.findall(line)\n",
    "        for palavra in palavras:\n",
    "            yield palavra.lower(), 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `mapper_get_words`:\n",
    "\n",
    "- Funciona de maneira similar ao **Job 2**, utilizando a expressão regular para identificar palavras no texto, removendo pontuações.\n",
    "- Para cada palavra encontrada, emite a palavra em minúsculas com o valor 1, indicando que a palavra apareceu uma vez.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Reducer1: `reducer_count_words`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def reducer_count_words(self, palavra, values):\n",
    "        yield palavra, sum(values)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `reducer_count_words`: \n",
    "\n",
    "- Soma todas as ocorrências de cada palavra (agrupadas pela palavra).\n",
    "- Emite a palavra e o total de vezes que ela apareceu no texto.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3. Maper2: `mapper_make_counts_key`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def mapper_make_counts_key(self, palavra, count):\n",
    "        yield '%04d' % int(count), palavra\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `mapper_make_counts_key`: \n",
    "\n",
    "- Recebe as palavras e suas contagens do primeiro mapeamento e inverte a relação chave-valor.\n",
    "- Aqui, a **contagem** passa a ser a chave, formatada como uma string de quatro dígitos (`'%04d' % int(count)`), enquanto a **palavra** se torna o valor.\n",
    "- O objetivo é permitir que a segunda fase de redução ordene as palavras com base em suas contagens.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 4. Reducer2: `reducer_output_words`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def reducer_output_words(self, count, palavras):\n",
    "        for palavra in palavras:\n",
    "            yield count, palavra\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `reducer_output_words`: \n",
    "\n",
    "- Recebe as contagens e as palavras agrupadas por essas contagens.\n",
    "- Emite a contagem e as palavras associadas a ela, garantindo que o resultado final esteja ordenado pela frequência das palavras.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### 8.2 Resumo\n",
    "\n",
    "- Este **job MapReduce** realiza **dois passos**:\n",
    "  - **1. Contagem das palavras** no texto, da mesma forma que o Job 2.\n",
    "  - **2. Organização das palavras por frequência**, colocando as palavras mais frequentes no topo.\n",
    "- O uso de duas fases de mapeamento e redução permite contar as palavras no primeiro passo e ordenar as contagens no segundo.\n",
    "- Esse job retorna as palavras organizadas em ordem decrescente de frequência, permitindo uma visão clara das palavras mais comuns no texto analisado.\n",
    "\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 9. Aplicando o job MapReduce 3\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, usaremos o arquivo Python `MR-DataMining-3.py` para aplicar o **job MapReduce**. Para isso basta ir ao diretório do arquivo e no terminal digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "    python MR-DataMining-3.py hdfs:///data_mining/OrgulhoePreconceito.txt -r hadoop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 9.1 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Job Completed Successfully**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado.\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "\"2203\"\t\"her\"\n",
    "\"3658\"\t\"and\"\n",
    "\"3729\"\t\"of\"\n",
    "\"4242\"\t\"to\"\n",
    "\"4507\"\t\"the\"\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: Neste exemplo, temos as palavras mais comuns no texto, como: \"the\" foi utilizada 4507 vezes, \"to\" foi utilizada 4242 vezes, \"of\" foi utilizada 3729 vezes.\n",
    "\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br><br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f41c2d",
   "metadata": {},
   "source": [
    "# <center>Laboratório 4 (Analisando Logs de Servidores Web)</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "# Objetivo do Laboratório 4\n",
    "\n",
    "<br>\n",
    "\n",
    "O objetivo deste laboratório é analisar os **logs de servidores web** para entender o comportamento dos usuários que acessam o site. Mais especificamente, vamos contar quantas conexões foram feitas por cada **endereço IP**, permitindo-nos identificar quais IPs estão gerando mais tráfego para o servidor.\n",
    "\n",
    "**Esta análise é útil para** detectar padrões de acesso, monitorar a atividade do servidor e identificar possíveis comportamentos anômalos, como acessos excessivos de determinados IPs, que podem indicar ataques ou outros problemas de desempenho.\n",
    "\n",
    "**IMPORTANTE** -> Para este laboratório iremos responder duas perguntas de negócio usando **dois métodos diferentes**:\n",
    "\n",
    "- `MRJob`\n",
    "- `Usando apenas o Hadoop Streaming`\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Pergunta de Negócio\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Quais foram os 20 principais IPs que acessam o servidor?** (Esta pergunta será respondida usando o *MRJob*)\n",
    "\n",
    "> **Quantas conexões foram feitas por cada endereço IP?** (Esta pergunta será respondida *usando apenas o Hadoop Streaming*)\n",
    "\n",
    "Estas questões nos ajudarão a identificar os principais IPs que acessam o servidor e a quantidade de vezes que esses IPs realizaram conexões, possibilitando insights sobre o tráfego do servidor e eventuais ações necessárias.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Sobre o Dataset\n",
    "\n",
    "<br>\n",
    "\n",
    "Para este laboratório, vamos utilizar um **conjunto de dados de logs de servidor web**, que contém informações sobre cada requisição feita ao servidor, incluindo o endereço IP do cliente, data e hora do acesso, o tipo de requisição (por exemplo, `GET` ou `POST`), o status da resposta do servidor, e o tamanho da resposta em bytes.\n",
    "\n",
    "### Informações sobre o Dataset:\n",
    "\n",
    "- **Tamanho do arquivo**: 504.941.532 bytes\n",
    "- **Quantidade de linhas**: 4.477.843\n",
    "- **Formato dos logs**: O formato é típico de logs de servidor web, contendo campos como endereço IP, data e hora do acesso, método HTTP, URI requisitado, código de resposta e tamanho da resposta.\n",
    "  \n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# <center><u>Iniciando o Laboratório 4</u></center>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "## 1. Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "1.1 **Iniciar o HDFS (NameNode, DataNode, SecondaryNameNode)**:\n",
    "   ```bash\n",
    "   start-dfs.sh  |  stop-dfs.sh\n",
    "   ```\n",
    "1.2 **Iniciar o YARN (ResourceManager, NodeManager)**:\n",
    "   ```bash\n",
    "   start-yarn.sh  |  stop-yarn.sh\n",
    "   ```\n",
    "1.3 **Verificando serviços**:\n",
    "   ```bash\n",
    "   jps\n",
    "   ```\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 2. Criando Pasta/Diretório com o nome de `logs` para o Laboratório no HDFS\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    hdfs dfs -mkdir /logs\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "2.1 **Lista os arquivos e diretórios no HDFS raiz**.\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "   hdfs dfs -ls /\n",
    "```\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 3. Copiando o Dataset do `Sistema Operacional Local` para dentro do `HDFS`:\n",
    "\n",
    "<br>\n",
    "\n",
    "**Para este laboratório precisaremos do arquivo `web_server.log`**.\n",
    "\n",
    "Abrir pasta onde está o arquivo a ser copiado, abrir um terminal e digitar o comando abaixo:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    hdfs dfs -put web_server.log /logs\n",
    "```\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 4. Criando Arquivo Python com o Job para o MapReduce 1 (Com MRJob)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `LogsIp.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "import re                     # Importa a biblioteca regex para trabalhar com expressões regulares\n",
    "from heapq import nlargest    # Importa nlargest para selecionar os 20 maiores valores\n",
    "\n",
    "class MRAvaliaFilme(MRJob):\n",
    "    \n",
    "    def mapper(self, key, line):\n",
    "        # Regex para extrair as partes do log\n",
    "        # Cada parte do padrão regex é configurada para capturar as diferentes informações da linha de log\n",
    "        log_pattern = re.compile(\n",
    "            # Captura o endereço IP: \\S+ significa \"qualquer sequência de caracteres que não seja espaço\"\n",
    "            r'^(?P<ip>\\S+) '           \n",
    "            # Captura o Identd do cliente: também é uma sequência sem espaços (geralmente '-')\n",
    "            r'(?P<identd>\\S+) ' \n",
    "            # Captura o nome do usuário autenticado (geralmente '-' se não autenticado)\n",
    "            r'(?P<user>\\S+) '          \n",
    "            # Captura o timestamp (data e hora): .*? captura qualquer coisa entre colchetes\n",
    "            r'\\[(?P<timestamp>.*?)\\] ' \n",
    "            # Captura a requisição HTTP: qualquer coisa entre aspas (exemplo: \"GET /index.html HTTP/1.1\")\n",
    "            r'\"(?P<request>.*?)\" '     \n",
    "            # Captura o código de status HTTP: \\d{3} significa \"exatamente três dígitos\" (ex: 200, 404)\n",
    "            r'(?P<status>\\d{3}) '    \n",
    "            # Captura o tamanho do objeto retornado: \\S+ captura qualquer sequência que não seja espaço (ex: 5120 bytes ou '-')\n",
    "            r'(?P<size>\\S+)'           \n",
    "        )\n",
    "\n",
    "        # Aplicar regex à linha\n",
    "        match = log_pattern.match(line)  # 'match' tenta aplicar o padrão à linha do log e retorna um objeto match se for bem-sucedido\n",
    "        \n",
    "        if match:\n",
    "            # Extrair o IP do objeto match\n",
    "            ip = match.group('ip')  # 'group' retorna a parte capturada pelo nome do grupo (neste caso, 'ip' é o endereço IP)\n",
    "            \n",
    "            # Emitir o IP com valor 1, indicando uma conexão\n",
    "            yield ip, 1  # O Mapper emite o endereço IP e o valor 1, que será somado no reducer\n",
    "\n",
    "    def reducer(self, ip, occurrences):\n",
    "        # Armazenar as contagens de IPs em uma lista\n",
    "        self.ip_counts = getattr(self, 'ip_counts', [])\n",
    "        self.ip_counts.append((ip, sum(occurrences)))\n",
    "\n",
    "    def reducer_final(self):\n",
    "        # Selecionar os 20 IPs com mais conexões\n",
    "        top_20_ips = nlargest(20, self.ip_counts, key=lambda x: x[1])\n",
    "        \n",
    "        # Emitir os 20 maiores IPs\n",
    "        for ip, total in top_20_ips:\n",
    "            yield ip, total\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRAvaliaFilme.run()  # Executa o job de MapReduce\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.1 Explicando Código do Job MapReduce 1 (`LogsIp.py`)\n",
    "\n",
    "<br>\n",
    "\n",
    "O código **LogsIp.py** implementa um **job MapReduce** que processa arquivos de log de servidor da web, com o objetivo de contar quantas vezes cada endereço IP aparece no log, o que representa o número de conexões feitas por esse IP. O job está dividido em duas partes principais: o **Mapper** e o **Reducer**, além de uma etapa de **ordenação no reducer_final para exibir apenas os 20 maiores IPs com mais conexões**.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Mapper:\n",
    "\n",
    "A função `mapper(self, key, line)` é responsável por ler as linhas do arquivo de log, extrair o endereço IP de cada linha usando uma expressão regular (regex) e emitir esse IP com o valor **1**, indicando que uma conexão foi feita por esse IP.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def mapper(self, key, line):\n",
    "        # Regex para extrair as partes do log\n",
    "        # Cada parte do padrão regex é configurada para capturar as diferentes informações da linha de log\n",
    "        log_pattern = re.compile(\n",
    "            # Captura o endereço IP: \\S+ significa \"qualquer sequência de caracteres que não seja espaço\"\n",
    "            r'^(?P<ip>\\S+) '           \n",
    "            # Captura o Identd do cliente: também é uma sequência sem espaços (geralmente '-')\n",
    "            r'(?P<identd>\\S+) ' \n",
    "            # Captura o nome do usuário autenticado (geralmente '-' se não autenticado)\n",
    "            r'(?P<user>\\S+) '          \n",
    "            # Captura o timestamp (data e hora): .*? captura qualquer coisa entre colchetes\n",
    "            r'\\[(?P<timestamp>.*?)\\] ' \n",
    "            # Captura a requisição HTTP: qualquer coisa entre aspas (exemplo: \"GET /index.html HTTP/1.1\")\n",
    "            r'\"(?P<request>.*?)\" '     \n",
    "            # Captura o código de status HTTP: \\d{3} significa \"exatamente três dígitos\" (ex: 200, 404)\n",
    "            r'(?P<status>\\d{3}) '    \n",
    "            # Captura o tamanho do objeto retornado: \\S+ captura qualquer sequência que não seja espaço (ex: 5120 bytes ou '-')\n",
    "            r'(?P<size>\\S+)'           \n",
    "        )\n",
    "\n",
    "        # Aplicar regex à linha\n",
    "        match = log_pattern.match(line)  # 'match' tenta aplicar o padrão à linha do log e retorna um objeto match se for bem-sucedido\n",
    "        \n",
    "        if match:\n",
    "            # Extrair o IP do objeto match\n",
    "            ip = match.group('ip')  # 'group' retorna a parte capturada pelo nome do grupo (neste caso, 'ip' é o endereço IP)\n",
    "            \n",
    "            # Emitir o IP com valor 1, indicando uma conexão\n",
    "            yield ip, 1  # O Mapper emite o endereço IP e o valor 1, que será somado no reducer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `mapper`:\n",
    "\n",
    "- **Recebe**: A função `mapper` recebe como parâmetros uma chave (`key`) e uma linha de log (`line`).\n",
    "- **Processa**: Ela aplica uma **expressão regular** (regex) à linha de log para identificar e capturar partes específicas da linha, como o endereço IP, o timestamp, o status HTTP, entre outros.\n",
    "- **Emite**: Quando a linha é válida (ou seja, corresponde ao padrão da regex), o `mapper` emite o **endereço IP** como chave e o número **1** como valor, indicando que uma conexão foi feita por aquele IP.\n",
    "\n",
    "Isso é o que chamamos de **mapeamento** no processo de MapReduce: associar cada IP com o número 1 para que no próximo passo, o reducer possa contar quantas vezes cada IP apareceu no log.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Reducer:\n",
    "\n",
    "A função `reducer(self, ip, occurrences)` acumula as contagens de IPs e, em um segundo estágio, a função `reducer_final(self)` seleciona e emite os 20 IPs com o maior número de conexões.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def reducer(self, ip, occurrences):\n",
    "        # Armazenar as contagens de IPs em uma lista\n",
    "        self.ip_counts = getattr(self, 'ip_counts', [])\n",
    "        self.ip_counts.append((ip, sum(occurrences)))\n",
    "\n",
    "    def reducer_final(self):\n",
    "        # Selecionar os 20 IPs com mais conexões\n",
    "        top_20_ips = nlargest(20, self.ip_counts, key=lambda x: x[1])\n",
    "        \n",
    "        # Emitir os 20 maiores IPs\n",
    "        for ip, total in top_20_ips:\n",
    "            yield ip, total\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `reducer`:\n",
    "\n",
    "- **Recebe**: O `reducer` recebe o **IP** (como chave) e uma lista de valores **1** (representando o número de vezes que o IP foi emitido pelo mapper).\n",
    "- **Acumula**: Em vez de emitir o resultado imediatamente, a função `reducer` acumula as contagens de IPs em uma lista `self.ip_counts` para processamento posterior.\n",
    "\n",
    "Isso é o que chamamos de **redução** no MapReduce: a consolidação de todas as ocorrências de um determinado IP em uma única soma.\n",
    "\n",
    "A função `reducer_final`:\n",
    "\n",
    "- **Processa**: No final da fase de redução, o `reducer_final` seleciona os **20 IPs** com mais conexões usando a função `nlargest` da biblioteca `heapq`, que é eficiente para encontrar os maiores valores.\n",
    "- **Emite**: Por fim, o `reducer_final` emite os **20 IPs com o maior número de conexões**.\n",
    "\n",
    "Isso é o que chamamos de **redução** e **seleção final** no MapReduce: a consolidação de todas as ocorrências de um determinado IP e a seleção dos 20 maiores IPs com base no número total de conexões.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.2 Resumo\n",
    "\n",
    "O código `LogsIp.py` implementa um job de **MapReduce** para contar o número de conexões feitas por cada **endereço IP** em um arquivo de log de servidor da web. O **mapper** extrai os IPs de cada linha do log e os associa ao valor `1`, enquanto o **reducer** acumula esses valores para calcular o número total de conexões feitas por cada IP. No final, o `reducer_final` seleciona e emite apenas os 20 maiores IPs com mais conexões.\n",
    "\n",
    "- **Mapper**: O mapper usa uma **expressão regular (regex)** para capturar o IP de cada linha de log e emite esse IP com o valor `1` (indicando uma conexão).\n",
    "- **Reduce**r: O reducer acumula todas as ocorrências de cada IP e, em uma fase final, seleciona os **20 IPs** com o maior número de conexões e os emite.\n",
    "\n",
    "Este job pode ser *útil para analisar o tráfego de um servidor*, identificando quais IPs acessaram o servidor com mais frequência e fornecendo uma visão sobre os maiores contribuintes de tráfego.\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 5. Aplicando o job MapReduce 1\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, usaremos o arquivo Python `LogsIp.py` para aplicar o **job MapReduce**. Para isso basta ir ao diretório do arquivo e no terminal digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "    python LogsIp.py hdfs:///logs/web_server.log -r hadoop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.1 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Job Completed Successfully**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado. No nosso exemplo, vemos a quantidade de filmes que receberam cada tipo de nota:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "\"10.216.113.172\"\t158613\n",
    "\"10.220.112.1\"\t51942\n",
    "\"10.173.141.213\"\t47503\n",
    "\"10.240.144.183\"\t43592\n",
    "\"10.41.69.177\"\t37554\n",
    "\"10.169.128.121\"\t22516\n",
    "\"10.211.47.159\"\t20866\n",
    "...\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: *\"10.216.113.172\" fez 158.613 conexões* ao servidor durante o período em que o log foi capturado, sendo o IP com mais conexões. A saída **exibe os 20 IPs com o maior número de conexões**, em ordem decrescente, de acordo com o que foi especificado no nosso `reducer_final`.\n",
    "\n",
    "<br><br><br> \n",
    "\n",
    "\n",
    "\n",
    "## 6. Criando Arquivos Python com o Job para o MapReduce 2 (Usando apenas o Hadoop Streaming)\n",
    "\n",
    "<br>\n",
    "\n",
    "Nesta seção, criamos os scripts `LogsIpMapper.py` e `LogsIpReducer.py` para realizar o processamento de logs usando o Hadoop Streaming, que é uma maneira flexível de implementar jobs MapReduce com qualquer linguagem de programação. Aqui, usaremos Python para implementar o Mapper e o Reducer para contar quantas conexões foram feitas por cada endereço IP.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `LogsIpMapper.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Regex para extrair o endereço IP\n",
    "log_pattern = re.compile(\n",
    "    r'^(?P<ip>\\S+) '           # Captura o endereço IP\n",
    "    r'(?P<identd>\\S+) '        # Ignora o Identd\n",
    "    r'(?P<user>\\S+) '          # Ignora o usuário\n",
    "    r'\\[(?P<timestamp>.*?)\\] ' # Ignora o timestamp\n",
    "    r'\"(?P<request>.*?)\" '     # Ignora a requisição\n",
    "    r'(?P<status>\\d{3}) '      # Ignora o status\n",
    "    r'(?P<size>\\S+)'           # Ignora o tamanho\n",
    ")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Aplicar regex à linha\n",
    "    match = log_pattern.match(line)\n",
    "    if match:\n",
    "        ip = match.group('ip')\n",
    "        print(ip)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `LogsIpReducer.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "current_ip_address = None\n",
    "current_ip_address_count = 0\n",
    "\n",
    "# Itera sobre cada linha recebida do Mapper\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    \n",
    "    # Verifica se a linha está vazia (ignora linhas vazias)\n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    # O Mapper emite apenas o IP, por isso espera-se uma única coluna\n",
    "    try:\n",
    "        new_ip_address = line\n",
    "    except ValueError:\n",
    "        # Se houver algum erro inesperado, continue\n",
    "        continue\n",
    "\n",
    "    # Se o IP atual mudar, imprime o IP anterior e sua contagem\n",
    "    if current_ip_address and current_ip_address != new_ip_address:\n",
    "        print(\"{0}\\t{1}\".format(current_ip_address, current_ip_address_count))\n",
    "        current_ip_address_count = 0\n",
    "\n",
    "    # Atualiza o IP atual e incrementa sua contagem\n",
    "    current_ip_address = new_ip_address\n",
    "    current_ip_address_count += 1\n",
    "\n",
    "# Após iterar, imprime o último IP e sua contagem\n",
    "if current_ip_address:\n",
    "    print(\"{0}\\t{1}\".format(current_ip_address, current_ip_address_count))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6.1 Explicando Códigos do Job MapReduce 2 (`LogsIpMapper.py` e `LogsIpReducer`)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. LogsIpMapper:\n",
    "\n",
    "O código `LogsIpMapper.py` implementa o **Mapper** do job MapReduce usando **Hadoop Streaming**. Ele é responsável por **extrair o endereço IP** de cada linha de log e emitir esse endereço para o Reducer.\n",
    "\n",
    "**Explicação:**\n",
    "- **Leitura de Entrada**: O Mapper lê cada linha de log via `sys.stdin` (a entrada padrão).\n",
    "- **Regex para Processamento**: Utiliza uma expressão regular (`log_pattern`) para identificar e extrair o endereço IP de cada linha de log. O regex captura o primeiro campo (IP) e ignora os outros campos como `identd`, `usuário`, `timestamp`, `requisição`, `status`, e `tamanho`.\n",
    "- **Emissão do IP**: Se a linha estiver no formato correto, o endereço IP é extraído e impresso, que é a saída do Mapper. Essa saída é usada como entrada no Reducer. O `print(ip)` emite o IP para o Hadoop Streaming, que será processado pelo Reducer.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. LogsIpReducer:\n",
    "\n",
    "O código `LogsIpReducer.py` implementa o **Reducer**, responsável por **contar a frequência** com que cada IP aparece nos logs.\n",
    "\n",
    "**Explicação:**\n",
    "- **Leitura da Entrada**: O Reducer lê os endereços IP gerados pelo Mapper, via `sys.stdin`.\n",
    "- **Contagem de IPs**: Ele conta quantas vezes o mesmo IP aparece consecutivamente. Cada vez que o IP muda, ele imprime o IP anterior junto com sua contagem.\n",
    "- **Emissão de Resultados**: No final, o último IP e sua contagem são emitidos.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6.2 Resumo\n",
    "\n",
    "Os scripts `LogsIpMapper.py` e `LogsIpReducer.py` implementam um **job MapReduce simples** utilizando o **Hadoop Streaming**. O **Mapper** lê os logs do servidor, extrai e emite os endereços IP. O **Reducer** conta o número de vezes que cada IP aparece e exibe o resultado final, mostrando quantas conexões foram feitas por cada endereço IP. Esse método é uma maneira eficiente de processar grandes arquivos de log de servidores e identificar padrões de acesso.\n",
    "\n",
    "Esse segundo método, utilizando apenas o **Hadoop Streaming**, nos permite responder à pergunta de **quantas conexões foram feitas por cada endereço IP** de forma eficiente e escalável.\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 7. Aplicando o job MapReduce 2\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, usaremos o arquivos Python `LogsIpMapper.py` e `LogsIpReducer` para aplicar o **job MapReduce** usando diretamente o **Hadoop Streaming**. \n",
    "\n",
    "O processo agora será **diferente** de quando usamos o *MRJob*. Para isso basta ir ao diretório do arquivo e no terminal digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "  hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.2.0.jar \\\n",
    "  -input /logs/web_server.log \\\n",
    "  -output /logs/output_ip_count \\\n",
    "  -mapper LogsIpMapper.py \\\n",
    "  -reducer LogsIpReducer.py \\\n",
    "  -file LogsIpMapper.py \\\n",
    "  -file LogsIpReducer.py\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Esse comando faz o seguinte:\n",
    "\n",
    "- **Hadoop Streaming Jar**: Chama o jar do Hadoop Streaming.\n",
    "- **Input/Output**: Especifica o arquivo de log (`/logs/web_server.log`) como entrada e o diretório de saída (`/logs/output_ip_count`) para os resultados.\n",
    "- **Mapper/Reducer**: Define o `LogsIpMapper.py` como Mapper e o `LogsIpReducer.py` como Reducer.\n",
    "- **Arquivos**: Especifica os arquivos Python que serão usados no processo (`-file LogsIpMapper.py -file LogsIpReducer.py`).\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 7.2 Verificando o Resultado\n",
    "\n",
    "<br>\n",
    "\n",
    "Após a execução bem-sucedida do job MapReduce, você pode verificar o resultado da contagem de IPs. Para isso, siga os passos abaixo:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. **Listar Arquivos de Saída**: Verifique o diretório de saída no HDFS para garantir que o arquivo foi gerado corretamente:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /logs/output_ip_count\n",
    "```\n",
    "\n",
    "Isso deve mostrar um ou mais arquivos `part-00000` que contêm os resultados do MapReduce.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Exibir o Conteúdo da Saída**: Para visualizar o conteúdo do arquivo de saída, use o comando cat no HDFS:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "hdfs dfs -cat /logs/output_ip_count/part-00000\n",
    "```\n",
    "\n",
    "Esse comando irá exibir a lista de IPs e o número de vezes que cada IP apareceu nos logs. O formato da saída será algo como:\n",
    "\n",
    "```bash\n",
    "192.168.0.1   15\n",
    "10.0.0.2      5\n",
    "172.16.0.3    20\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: O primeiro valor é o endereço IP e o segundo valor é o número de vezes que o IP apareceu.\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br><br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512c438",
   "metadata": {},
   "source": [
    "# <center>Laboratório 5 (Análise de Tickets de Estacionamento)</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "# Objetivo do Laboratório 5\n",
    "\n",
    "<br>\n",
    "\n",
    "O objetivo deste laboratório é utilizar um **dataset de multas de estacionamento da cidade de Nova York** para realizar uma **análise abrangente dos padrões de infração**. A análise ajudará a **identificar as infrações mais comuns**, **os horários mais propensos à emissão de multas**, os **tipos de veículos mais infratores**, e outras métricas relevantes. Com base nos insights obtidos, será possível fazer recomendações para melhorar a regulamentação de estacionamento e reduzir as infrações.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Pergunta de Negócio\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Quais são as infrações de estacionamento mais comuns em diferentes bairros?**\n",
    "\n",
    "- Identificar os bairros com maior número de infrações e as respectivas violações mais frequentes.\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Em quais horários ocorrem a maioria das infrações de estacionamento?**\n",
    "\n",
    "- Analisar os horários mais propensos à emissão de multas e sugerir mudanças nos horários de fiscalização.\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Quais tipos de veículos (carro, SUV, caminhonete) estão mais propensos a receber multas?**\n",
    "\n",
    "- Examinar o tipo de veículo e a frequência das infrações para entender quais tipos estão mais sujeitos às penalidades.\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Quais são as ruas mais propensas a receber multas por estacionamento irregular?**\n",
    "\n",
    "- Analisar as ruas com maior incidência de multas e recomendar melhorias na sinalização ou nos regulamentos.\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Quais são códigos de rua mais propensos a receber multas por estacionamento irregular?**\n",
    "\n",
    "- Analisar códigos de ruas com maior incidência de multas e recomendar melhorias na sinalização ou nos regulamentos.\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Quais são as marcas de veículos mais propensas a receber multas por estacionamento irregular?**\n",
    "\n",
    "- Examinar a marca do veículo e a frequência das infrações para entender quais tipos estão mais sujeitos às penalidades.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "# Sobre o Dataset\n",
    "\n",
    "<br>\n",
    "\n",
    "Para este laboratório, utilizaremos o **dataset NYC Parking Tickets**, que contém dados detalhados sobre multas de estacionamento emitidas em Nova York. O dataset inclui informações sobre a localização, o tipo de infração, o tipo de veículo e outras características relevantes. A análise será focada em identificar padrões e fornecer insights práticos para a gestão de estacionamento na cidade.\n",
    "\n",
    "#### Descrição Geral do Novo Dataset\n",
    "\n",
    "- **Número de registros**: Milhões de registros (número exato depende da amostra utilizada).\n",
    "- **Colunas**: `51 colunas` detalhando as infrações de estacionamento, tais como:\n",
    "  - **Summons Number**: Número da multa.\n",
    "  - **Plate ID**: Número da placa do veículo.\n",
    "  - **Registration State**: Estado onde o veículo foi registrado.\n",
    "  - **Issue Date**: Data da infração.\n",
    "  - **Violation Code**: Código da violação cometida.\n",
    "  - **Vehicle Body Type**: Tipo de carroceria do veículo.\n",
    "  - **Vehicle Make**: Marca do veículo.\n",
    "  - **Violation Location**: Localização da violação (bairro, rua).\n",
    "  - **Violation Time**: Hora em que a infração foi registrada.\n",
    "  - **Vehicle Color**, **Year**, **Meter Number**, **Latitude**, **Longitude**, entre outros.\n",
    "\n",
    "<br>\n",
    "\n",
    "Essa estrutura permitirá análises detalhadas sobre o comportamento das infrações de estacionamento em Nova York, bem como a possibilidade de identificar tendências relevantes para o gerenciamento de trânsito.\n",
    "  \n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# <center><u>Iniciando o Laboratório 5</u></center>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "## 1. Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "1.1 **Iniciar o HDFS (NameNode, DataNode, SecondaryNameNode)**:\n",
    "   ```bash\n",
    "   start-dfs.sh  |  stop-dfs.sh\n",
    "   ```\n",
    "1.2 **Iniciar o YARN (ResourceManager, NodeManager)**:\n",
    "   ```bash\n",
    "   start-yarn.sh  |  stop-yarn.sh\n",
    "   ```\n",
    "1.3 **Verificando serviços**:\n",
    "   ```bash\n",
    "   jps\n",
    "   ```\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 2. Criando Pasta/Diretório com o nome de `tickets` para o Laboratório no HDFS\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    hdfs dfs -mkdir /tickets\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "2.1 **Lista os arquivos e diretórios no HDFS raiz**.\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "   hdfs dfs -ls /\n",
    "```\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 3. Copiando o Dataset do `Sistema Operacional Local` para dentro do `HDFS`:\n",
    "\n",
    "<br>\n",
    "\n",
    "**Para este laboratório precisaremos do arquivo `PV_2016.csv`**.\n",
    "\n",
    "Abrir pasta onde está o arquivo a ser copiado, abrir um terminal e digitar o comando abaixo:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    hdfs dfs -put PV_2016.csv /tickets\n",
    "```\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> <br> <br>\n",
    "\n",
    "## 4. <u>Criando Arquivo Python com o Job para o MapReduce 1</u>\n",
    "\n",
    "<br>\n",
    "\n",
    "> Quais são as infrações de estacionamento mais comuns em diferentes bairros?\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `CodeTicket1.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class InfracoesPorBairro(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_bairro_infracao,\n",
    "                   reducer=self.reducer_count_infracoes),\n",
    "            MRStep(reducer=self.reducer_find_max_infracoes)\n",
    "        ]\n",
    "\n",
    "    # Mapper: Extrai o bairro e o código da infração de cada linha\n",
    "    def mapper_get_bairro_infracao(self, _, line):\n",
    "        # Usando vírgula como delimitador (formato CSV)\n",
    "        fields = line.split(',')\n",
    "        \n",
    "        # Ignorando o cabeçalho (garantindo que os campos são numéricos)\n",
    "        if fields[0].isdigit():  \n",
    "            try:\n",
    "                # Extrair o bairro (Violation Location) e o código da infração (Violation Code)\n",
    "                bairro = fields[14].strip()  # Posição 14 para Violation Location\n",
    "                codigo_infracao = fields[5].strip()  # Posição 5 para Violation Code\n",
    "                \n",
    "                # Verificando se o bairro está vazio\n",
    "                if not bairro:\n",
    "                    return\n",
    "                \n",
    "                # Se o código de infração estiver ausente, defina como 'unknown'\n",
    "                if not codigo_infracao:\n",
    "                    codigo_infracao = 'unknown'\n",
    "                \n",
    "                # Emitir o bairro e o código da infração como chave, e o valor 1 para contar\n",
    "                yield (bairro, codigo_infracao), 1\n",
    "                \n",
    "            except IndexError:\n",
    "                # Caso ocorra erro de índice, a linha pode estar malformada e será ignorada\n",
    "                return\n",
    "\n",
    "    # Reducer: Soma as ocorrências de cada (bairro, tipo de infração)\n",
    "    def reducer_count_infracoes(self, key, values):\n",
    "        yield key[0], (sum(values), key[1])\n",
    "\n",
    "    # Reducer final: Determina a infração mais comum por bairro\n",
    "    def reducer_find_max_infracoes(self, bairro, infracoes_contadas):\n",
    "        yield bairro, max(infracoes_contadas)  # max() retorna a infração mais frequente\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    InfracoesPorBairro.run()\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.1 Explicando Código do Job MapReduce 1 (`CodeTicket1.py`)\n",
    "\n",
    "<br>\n",
    "\n",
    "O script `CodeTicket1` implementa um job MapReduce que busca encontrar as infrações de estacionamento mais comuns em diferentes bairros da cidade de Nova York. Ele realiza isso em dois passos principais: primeiro, ele mapeia os dados para extrair a localização do bairro e o código de infração; em seguida, agrega e reduz os dados para identificar qual infração foi mais frequente em cada bairro.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Mapper:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Mapper: Extrai o bairro e o código da infração de cada linha\n",
    "    def mapper_get_bairro_infracao(self, _, line):\n",
    "        # Usando vírgula como delimitador (formato CSV)\n",
    "        fields = line.split(',')\n",
    "        \n",
    "        # Ignorando o cabeçalho (garantindo que os campos são numéricos)\n",
    "        if fields[0].isdigit():  \n",
    "            try:\n",
    "                # Extrair o bairro (Violation Location) e o código da infração (Violation Code)\n",
    "                bairro = fields[14].strip()  # Posição 14 para Violation Location\n",
    "                codigo_infracao = fields[5].strip()  # Posição 5 para Violation Code\n",
    "                \n",
    "                # Verificando se o bairro está vazio\n",
    "                if not bairro:\n",
    "                    return\n",
    "                \n",
    "                # Se o código de infração estiver ausente, defina como 'unknown'\n",
    "                if not codigo_infracao:\n",
    "                    codigo_infracao = 'unknown'\n",
    "                \n",
    "                # Emitir o bairro e o código da infração como chave, e o valor 1 para contar\n",
    "                yield (bairro, codigo_infracao), 1\n",
    "                \n",
    "            except IndexError:\n",
    "                # Caso ocorra erro de índice, a linha pode estar malformada e será ignorada\n",
    "                return\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `mapper_get_bairro_infracao` é responsável por:\n",
    "\n",
    "- **Dividir a linha de entrada**: A função começa dividindo cada linha de entrada usando a tabulação (`'\\t'`) como delimitador para separar os campos.\n",
    "    - **Ignorar o cabeçalho**: O `mapper` utiliza a verificação `if fields[0].isdigit()` para garantir que está processando apenas linhas de dados (onde o campo Summons Number é numérico) e não o cabeçalho, que deve ser ignorado.\n",
    "- **Extrair os campos de interesse**: Em seguida, tenta extrair os campos `'Violation Location'` (bairro) (bairro), que está na posição `14` da linha e `'Violation Code'` (código da infração) que está na posição `5` da linha.. Se esses campos não forem encontrados ou estiverem ausentes, a linha é ignorada usando o bloco `try-except`.\n",
    "- **Verificar valores nulos**:\n",
    "  - Se o valor do campo **'Violation Location'** for nulo ou ausente, a linha é descartada.\n",
    "  - Se o valor do campo **'Violation Code'** for nulo ou ausente, o código de infração é substituído por `'unknown'`.\n",
    "- **Emitir chave-valor**: O `mapper` emite um par chave-valor onde a chave é a combinação (`bairro, código de infração`) e o valor é sempre `1`, indicando que uma infração ocorreu.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Reducer:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Reducer: Soma as ocorrências de cada (bairro, tipo de infração)\n",
    "    def reducer_count_infracoes(self, key, values):\n",
    "        yield key[0], (sum(values), key[1])\n",
    "\n",
    "    # Reducer final: Determina a infração mais comum por bairro\n",
    "    def reducer_find_max_infracoes(self, bairro, infracoes_contadas):\n",
    "        yield bairro, max(infracoes_contadas)  # max() retorna a infração mais frequente\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `reducer_count_infracoes` é responsável por:\n",
    "\n",
    "- **Receber os dados do mapper**: Ela recebe a chave (`bairro, código da infração`) e os valores que representam o número de ocorrências desse par.\n",
    "- **Contar as infrações**: A função soma todas as ocorrências (`sum(values)`) para cada combinação de bairro e código de infração, resultando no total de infrações desse tipo em determinado bairro.\n",
    "- **Emitir chave-valor**: O `reducer` então emite o bairro (`key[0]`) como a nova chave, e como valor, emite uma tupla contendo o número total de ocorrências e o código da infração correspondente.\n",
    "  - O primeiro elemento da tupla é o número total de ocorrências.\n",
    "  - O segundo elemento da tupla é o código da infração correspondente (`key[1]`).\n",
    "\n",
    "<br>\n",
    "\n",
    "A função `reducer_find_max_infracoes` é responsável por:\n",
    "\n",
    "- **Receber os dados do primeiro reducer**: Ela recebe o nome do bairro e uma lista de tuplas, onde cada tupla contém a contagem de infrações e o código da infração.\n",
    "- **Determinar a infração mais comum**: Utiliza a função `max(infracoes_contadas)` para encontrar a tupla com o maior número de ocorrências. Isso identifica a infração mais comum em cada bairro.\n",
    "- **Emitir chave-valor**: A função emite o bairro como chave e a tupla (`maior contagem, código da infração`) como valor, onde a infração mais frequente é retornada para cada bairro.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.2 Resumo\n",
    "\n",
    "- **Pipeline de duas etapas**:\n",
    "  - **Primeira etapa (mapper e reducer)**: O mapeamento agrupa as infrações por bairro e tipo, e o primeiro `reducer` soma as ocorrências de cada tipo de infração em cada bairro.\n",
    "  - **Segunda etapa (reducer final)**: O `reducer` final processa as infrações agregadas para encontrar a mais comum em cada bairro.\n",
    "- **Funcionamento geral**:\n",
    "  - O `mapper` extrai os bairros e os códigos das infrações, agrupando as ocorrências.\n",
    "  - O primeiro `reducer` conta o número de infrações para cada combinação de bairro e código de infração.\n",
    "  - O segundo `reducer` identifica a infração mais comum em cada bairro, retornando o código e a contagem dessa infração.\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 5. Aplicando o job MapReduce 1\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, usaremos o arquivo Python `CodeTicket1.py` para aplicar o **job MapReduce**. Para isso basta ir ao diretório do arquivo e no terminal digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "    python CodeTicket1.py hdfs:///tickets/PV_2016.csv -r hadoop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.1 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Job Completed Successfully**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado. No nosso exemplo, vemos a quantidade de filmes que receberam cada tipo de nota:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "\"0\"\t[1253511, \"36\"]\n",
    "\"1\"\t[72056, \"14\"]\n",
    "\"10\"\t[26544, \"14\"]\n",
    "\"100\"\t[5533, \"21\"]\n",
    "\"101\"\t[6391, \"21\"]\n",
    "...\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: \n",
    "\n",
    "- Cada linha exibe o bairro (por exemplo, `\"0\"` ou `\"1\"`) e uma tupla com o número total de infrações associadas àquele bairro (por exemplo, `1253511 infrações` para o bairro `\"0\"`).\n",
    "- O segundo elemento da tupla é o código da infração mais comum naquele bairro (por exemplo, o código `\"36\"` para o bairro `\"0\"`).\n",
    "\n",
    "Esses resultados indicam que o MapReduce identificou corretamente a infração mais comum em cada bairro e contou o número total de infrações para cada combinação de bairro e infração.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "## 6. <u>Criando Arquivo Python com o Job para o MapReduce 2</u>\n",
    "\n",
    "<br>\n",
    "\n",
    "> Em quais horários ocorrem a maioria das infrações de estacionamento?\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `CodeTicket2.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class InfracoesPorTempo(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_tempo_infracao,\n",
    "                   reducer=self.reducer_count_tempo),\n",
    "            MRStep(reducer=self.reducer_top_30_tempos)\n",
    "        ]\n",
    "\n",
    "    # Mapper: Extrai o tempo da infração de cada linha\n",
    "    def mapper_get_tempo_infracao(self, _, line):\n",
    "        fields = line.split(',')\n",
    "        \n",
    "        if fields[0].isdigit():\n",
    "            try:\n",
    "                # Extrair o tempo (Violation Time)\n",
    "                tempo = fields[19].strip()  # Posição 19 para Violation Time\n",
    "                \n",
    "                if not tempo:\n",
    "                    tempo = 'unknown'\n",
    "                \n",
    "                yield tempo, 1\n",
    "                \n",
    "            except IndexError:\n",
    "                return\n",
    "\n",
    "    # Reducer: Soma as ocorrências de cada tempo\n",
    "    def reducer_count_tempo(self, key, values):\n",
    "        yield None, (sum(values), key)\n",
    "\n",
    "    # Reducer final: Seleciona os 30 tempos com mais ocorrências\n",
    "    def reducer_top_30_tempos(self, _, tempo_contagem):\n",
    "        # Ordena pela contagem (primeiro valor na tupla) e seleciona os 30 maiores\n",
    "        sorted_tempos = sorted(tempo_contagem, reverse=True, key=lambda x: x[0])\n",
    "        for count, tempo in sorted_tempos[:30]:\n",
    "            yield tempo, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    InfracoesPorTempo.run()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6.1 Explicando Código do Job MapReduce 2 (`CodeTicket2.py`)\n",
    "\n",
    "<br>\n",
    "\n",
    "O script `CodeTicket2` foi criado para responder à pergunta: **Em quais horários ocorrem a maioria das infrações de estacionamento?**. Ele processa os dados das infrações e seleciona os 30 horários com o maior número de ocorrências, exibindo os resultados em ordem decrescente.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Mapper:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Mapper: Extrai o tempo da infração de cada linha\n",
    "    def mapper_get_tempo_infracao(self, _, line):\n",
    "        fields = line.split(',')\n",
    "        \n",
    "        if fields[0].isdigit():\n",
    "            try:\n",
    "                # Extrair o tempo (Violation Time)\n",
    "                tempo = fields[19].strip()  # Posição 19 para Violation Time\n",
    "                \n",
    "                if not tempo:\n",
    "                    tempo = 'unknown'\n",
    "                \n",
    "                yield tempo, 1\n",
    "                \n",
    "            except IndexError:\n",
    "                return\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `mapper_get_tempo_infracao`:\n",
    "\n",
    "- **Dividir a linha de entrada**: A função divide a linha de entrada em campos separados por vírgula.\n",
    "- **Verificar se não é o cabeçalho**: A função verifica se o primeiro campo da linha é numérico, descartando o cabeçalho.\n",
    "- **Extrair o tempo de infração**: A função extrai o valor da coluna de **Violation Time** (posição 19).\n",
    "- **Verificar e corrigir valores ausentes**: Se o tempo estiver ausente, ele é marcado como 'unknown'.\n",
    "- **Emitir chave-valor**: A chave é o tempo da infração, e o valor é `1`, indicando uma ocorrência. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Reducer:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Reducer: Soma as ocorrências de cada tempo\n",
    "    def reducer_count_tempo(self, key, values):\n",
    "        yield None, (sum(values), key)\n",
    "\n",
    "    # Reducer final: Seleciona os 30 tempos com mais ocorrências\n",
    "    def reducer_top_30_tempos(self, _, tempo_contagem):\n",
    "        # Ordena pela contagem (primeiro valor na tupla) e seleciona os 30 maiores\n",
    "        sorted_tempos = sorted(tempo_contagem, reverse=True, key=lambda x: x[0])\n",
    "        for count, tempo in sorted_tempos[:30]:\n",
    "            yield tempo, count\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `reducer_count_tempo` é responsável por:\n",
    "\n",
    "- **Receber o tempo da infração**: Ele recebe o tempo como chave e as ocorrências como valores.\n",
    "- **Somar as ocorrências**: A função soma as ocorrências para cada tempo.\n",
    "- **Emitir chave-valor**: Emite um par onde a chave é `None` (para agrupar os valores no próximo passo) e o valor é uma tupla contendo o número total de ocorrências e o tempo da infração. \n",
    "  - **Por que usar None?** O `None` faz com que todos os pares (`contagem, tempo`) sejam enviados para o **próximo reducer** como uma única lista, permitindo que o segundo reducer tenha acesso a todas as ocorrências de tempos para fazer a ordenação.\n",
    "\n",
    "<br>\n",
    "\n",
    "A função `reducer_top_30_tempos` é responsável por:\n",
    "\n",
    "- **Ordenar e selecionar**: Ordena os tempos de infração pela contagem de ocorrências e seleciona os 30 maiores.\n",
    "- **Emitir os resultados**: Exibe os 30 tempos com mais infrações, em ordem decrescente.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 6.2 Resumo\n",
    "\n",
    "- O **mapper** extrai o horário das infrações.\n",
    "- O primeiro **reducer** soma o número de infrações em cada horário.\n",
    "- O segundo **reducer** ordena e seleciona os 30 horários com mais ocorrências.\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 7. Aplicando o job MapReduce 2\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, usaremos o arquivo Python `CodeTicket2.py` para aplicar o **job MapReduce**. Para isso basta ir ao diretório do arquivo e no terminal digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "    python CodeTicket2.py hdfs:///tickets/PV_2016.csv -r hadoop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 7.1 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Job Completed Successfully**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado. No nosso exemplo, vemos a quantidade de filmes que receberam cada tipo de nota:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "\"0836A\"\t30688\n",
    "\"1136A\"\t30288\n",
    "\"1140A\"\t28297\n",
    "\"0936A\"\t26267\n",
    "\"0840A\"\t25891\n",
    "\"1138A\"\t25674\n",
    "\"0906A\"\t25633\n",
    "\"0940A\"\t25485\n",
    "\"1145A\"\t25461\n",
    "\"1139A\"\t25388\n",
    "\"1137A\"\t24926\n",
    "\"0806A\"\t24790\n",
    "\"1142A\"\t24640\n",
    "\"1141A\"\t24302\n",
    "\"0945A\"\t24101\n",
    "\"0838A\"\t24037\n",
    "\"1143A\"\t23657\n",
    "\"1150A\"\t23597\n",
    "\"0845A\"\t23573\n",
    "\"0839A\"\t23551\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: O job identificou com sucesso os horários com maior número de infrações de estacionamento, e ordenou os 30 horários mais comuns, exibindo-os do mais alto para o mais baixo em termos de ocorrência. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "## 8. <u>Criando Arquivo Python com o Job para o MapReduce 3</u>\n",
    "\n",
    "<br>\n",
    "\n",
    "> Quais tipos de veículos (carro, SUV, caminhonete) estão mais propensos a receber multas?\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `CodeTicket3.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class InfracoesPorTipoVeiculo(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_veiculo_infracao,\n",
    "                   reducer=self.reducer_count_infracoes),\n",
    "            MRStep(reducer=self.reducer_top_veiculos)\n",
    "        ]\n",
    "\n",
    "    # Mapper: Extrai o tipo de veículo de cada linha\n",
    "    def mapper_get_veiculo_infracao(self, _, line):\n",
    "        fields = line.split(',')\n",
    "        \n",
    "        if fields[0].isdigit():\n",
    "            try:\n",
    "                # Extrair o tipo de veículo (Vehicle Body Type)\n",
    "                tipo_veiculo = fields[6].strip().upper() if fields[6].strip() else \"unknown\"  # Posição 6 para Vehicle Body Type\n",
    "                \n",
    "                # Emitir o tipo de veículo (substitui por 'unknown' se estiver ausente/nulo)\n",
    "                yield tipo_veiculo, 1\n",
    "                \n",
    "            except IndexError:\n",
    "                return\n",
    "\n",
    "    # Reducer: Soma as ocorrências de cada tipo de veículo\n",
    "    def reducer_count_infracoes(self, key, values):\n",
    "        yield None, (sum(values), key)\n",
    "\n",
    "    # Reducer final: Seleciona os tipos de veículos com mais infrações\n",
    "    def reducer_top_veiculos(self, _, tipo_veiculo_contagem):\n",
    "        # Ordena pela contagem e exibe todos os tipos de veículos relevantes\n",
    "        sorted_veiculos = sorted(tipo_veiculo_contagem, reverse=True, key=lambda x: x[0])\n",
    "        for count, tipo_veiculo in sorted_veiculos:\n",
    "            yield tipo_veiculo, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    InfracoesPorTipoVeiculo.run()\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 8.1 Explicando Código do Job MapReduce 3 (`CodeTicket3.py`)\n",
    "\n",
    "<br>\n",
    "\n",
    "O script `CodeTicket3` é um job MapReduce que identifica quais tipos de veículos (carro, SUV, caminhonete, etc.) estão mais propensos a receber multas de estacionamento com base nos dados da coluna `Vehicle Body Type`. Ele realiza essa análise em duas etapas: um mapeamento inicial para contar as ocorrências de cada tipo de veículo e, em seguida, um segundo estágio que ordena os tipos de veículos por número de multas.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Mapper:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    " # Mapper: Extrai o tipo de veículo de cada linha\n",
    "    def mapper_get_veiculo_infracao(self, _, line):\n",
    "        fields = line.split(',')\n",
    "        \n",
    "        if fields[0].isdigit():\n",
    "            try:\n",
    "                # Extrair o tipo de veículo (Vehicle Body Type)\n",
    "                tipo_veiculo = fields[6].strip().upper() if fields[6].strip() else \"unknown\"  # Posição 6 para Vehicle Body Type\n",
    "                \n",
    "                # Emitir o tipo de veículo (substitui por 'unknown' se estiver ausente/nulo)\n",
    "                yield tipo_veiculo, 1\n",
    "                \n",
    "            except IndexError:\n",
    "                return\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `mapper_get_veiculo_infracao`\n",
    "\n",
    "- **Processa cada linha**: Ela divide as linhas do dataset por vírgulas e extrai o campo `Vehicle Body Type` (posição 6) para identificar o tipo de veículo.\n",
    "- **Verifica valores ausentes**: Se o campo estiver ausente ou for nulo, o tipo de veículo será substituído por `\"unknown\"`.\n",
    "- **Emite o tipo de veículo**: O Mapper gera uma chave-valor, onde a chave é o tipo de veículo e o valor é `1`, indicando que esse tipo de veículo recebeu uma multa.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Reducer:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Reducer: Soma as ocorrências de cada tipo de veículo\n",
    "    def reducer_count_infracoes(self, key, values):\n",
    "        yield None, (sum(values), key)\n",
    "\n",
    "    # Reducer final: Seleciona os tipos de veículos com mais infrações\n",
    "    def reducer_top_veiculos(self, _, tipo_veiculo_contagem):\n",
    "        # Ordena pela contagem e exibe todos os tipos de veículos relevantes\n",
    "        sorted_veiculos = sorted(tipo_veiculo_contagem, reverse=True, key=lambda x: x[0])\n",
    "        for count, tipo_veiculo in sorted_veiculos:\n",
    "            yield tipo_veiculo, count\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `reducer_count_infracoes`:\n",
    "\n",
    "- **Soma as ocorrências**: Ela recebe os tipos de veículos e soma o número de ocorrências para cada tipo.\n",
    "- **Emite uma lista de ocorrências**: O Reducer emite uma tupla, onde o primeiro valor é a contagem total de multas e o segundo valor é o tipo de veículo.\n",
    "\n",
    "<br>\n",
    "\n",
    "A função `reducer_top_veiculos`:\n",
    "\n",
    "- **Ordena os veículos**: A função ordena os veículos pelo número de infrações, do maior para o menor.\n",
    "- **Emite o resultado**: O Reducer final emite os tipos de veículos ordenados pela quantidade de multas.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 8.2 Resumo\n",
    "\n",
    "- **Mapper**: Extrai o tipo de veículo da coluna `Vehicle Body Type` e emite esse valor com o número de ocorrências.\n",
    "- **Reducer**: Soma as ocorrências para cada tipo de veículo e ordena os resultados, apresentando os tipos de veículos mais comuns entre as multas.\n",
    "\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 9. Aplicando o job MapReduce 3\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, usaremos o arquivo Python `CodeTicket3.py` para aplicar o **job MapReduce**. Para isso basta ir ao diretório do arquivo AmigosIdade.py e no terminal digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "    python CodeTicket3.py hdfs:///tickets/PV_2016.csv -r hadoop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 9.1 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Job Completed Successfully**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado. No nosso exemplo, vemos a quantidade de filmes que receberam cada tipo de nota:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "\"SUBN\"\t3466020\n",
    "\"4DSD\"\t2992093\n",
    "\"VAN\"\t1518294\n",
    "\"DELV\"\t755274\n",
    "\"SDN\"\t424043\n",
    "\"2DSD\"\t276455\n",
    "\"PICK\"\t264271\n",
    "\"REFG\"\t84019\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: \n",
    "\n",
    "- O tipo de veículo `SUBN` (SUV) recebeu 3.466.020 multas, seguido por `4DSD` (sedan de 4 portas), que recebeu 2.992.093 multas.\n",
    "- Os veículos `VAN`, `DELV` (entrega), e `SDN` (sedan) também aparecem frequentemente na lista de veículos multados. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "## 10. <u>Criando Arquivo Python com o Job para o MapReduce 4</u>\n",
    "\n",
    "<br>\n",
    "\n",
    "> Quais são as marcas de veículos mais propensas a receber multas por estacionamento irregular?\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `CodeTicket4.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class InfracoesPorMarcaVeiculo(MRJob):\n",
    "\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_marca_veiculo_infracao,\n",
    "                   reducer=self.reducer_count_infracoes),\n",
    "            MRStep(reducer=self.reducer_top_veiculos)\n",
    "        ]\n",
    "\n",
    "    # Mapper: Extrai a marca de veículo de cada linha\n",
    "    def mapper_get_marca_veiculo_infracao(self, _, line):\n",
    "\n",
    "    \tfields = line.split(',')\n",
    "\n",
    "    \tif fields[0].isdigit():\n",
    "    \t\ttry:\n",
    "    \t\t\t# Extrair a marca de veículo (Vehicle Make)\n",
    "    \t\t\tmarca = fields[7].strip().upper() if fields[7].strip() else \"unknown\"\n",
    "\n",
    "    \t\t\t# Emitir a marca do veículo\n",
    "    \t\t\tyield marca, 1\n",
    "\n",
    "    \t\texcept IndexError:\n",
    "    \t\t\treturn\n",
    "\n",
    "    # Reducer: Soma as ocorrências de cada marca de veículo\n",
    "    def reducer_count_infracoes(self, key, values):\n",
    "    \tyield None, (sum(values), key)\n",
    "\n",
    "\n",
    "    # Reducer final: Seleciona os tipos de veículos com mais infrações\n",
    "    def reducer_top_veiculos(self, _, marca_veiculo_contagem):\n",
    "\n",
    "    \t# Ordena pela contagem e exibe todos as marcas de veiculos\n",
    "    \tsorted_veiculos = sorted(marca_veiculo_contagem, reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    \tfor count, marca in sorted_veiculos:\n",
    "    \t\tyield marca, count\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    InfracoesPorMarcaVeiculo.run()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 10.1 Explicando Código do Job MapReduce 4 (`CodeTicket4.py`)\n",
    "\n",
    "<br>\n",
    "\n",
    "O script `CodeTicket4` tem como objetivo identificar as marcas de veículos mais propensas a receber multas de estacionamento irregular, utilizando o formato de MapReduce. Ele segue duas etapas principais: mapeamento e redução. A seguir, detalharemos cada etapa.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Mapper:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Mapper: Extrai a marca de veículo de cada linha\n",
    "    def mapper_get_marca_veiculo_infracao(self, _, line):\n",
    "\n",
    "    \tfields = line.split(',')\n",
    "\n",
    "    \tif fields[0].isdigit():\n",
    "    \t\ttry:\n",
    "    \t\t\t# Extrair a marca de veículo (Vehicle Make)\n",
    "    \t\t\tmarca = fields[7].strip().upper() if fields[7].strip() else \"unknown\"\n",
    "\n",
    "    \t\t\t# Emitir a marca do veículo\n",
    "    \t\t\tyield marca, 1\n",
    "\n",
    "    \t\texcept IndexError:\n",
    "    \t\t\treturn\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `mapper_get_marca_veiculo_infracao`\n",
    "\n",
    "- **Divisão da linha**: A função divide cada linha do dataset utilizando a vírgula como delimitador (`line.split(',')`).\n",
    "- **Validação de campos**: Ela verifica se a linha processada é válida, assegurando que o campo `Summons Number` (posição 0) seja numérico.\n",
    "- **Extração da marca**: A marca do veículo é extraída da posição 7 (`Vehicle Make`). Se o campo estiver vazio ou nulo, a marca é substituída pelo valor `\"unknown\"`.\n",
    "- **Emitir chave-valor**: A função emite um par chave-valor, onde a chave é a marca do veículo e o valor é `1`, indicando uma ocorrência.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Reducer:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Reducer: Soma as ocorrências de cada marca de veículo\n",
    "    def reducer_count_infracoes(self, key, values):\n",
    "    \tyield None, (sum(values), key)\n",
    "\n",
    "\n",
    "    # Reducer final: Seleciona os tipos de veículos com mais infrações\n",
    "    def reducer_top_veiculos(self, _, marca_veiculo_contagem):\n",
    "\n",
    "    \t# Ordena pela contagem e exibe todos as marcas de veiculos\n",
    "    \tsorted_veiculos = sorted(marca_veiculo_contagem, reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    \tfor count, marca in sorted_veiculos:\n",
    "    \t\tyield marca, count\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `reducer_count_infracoes`:  Esta função reduz os dados somando todas as ocorrências para cada marca de veículo, agrupando-as por marca.\n",
    "\n",
    "- **Entrada**: A marca do veículo como chave e a lista de valores (contagens individuais).\n",
    "- **Saída**: Uma tupla contendo a soma das infrações e a marca do veículo.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "A função `reducer_top_veiculos`: Esta função final ordena os resultados por número de infrações (da maior para a menor) e exibe as marcas com o maior número de multas.\n",
    "\n",
    "- **Entrada**: A tupla com a contagem de infrações e a marca.\n",
    "- **Ordenação**: Os resultados são ordenados pela contagem (`x[0]`), de forma decrescente.\n",
    "- **Saída**: O resultado final exibe a marca e o número de multas recebidas.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### 10.2 Resumo\n",
    "\n",
    "- Este código segue a abordagem MapReduce para processar grandes quantidades de dados de multas de estacionamento, respondendo à pergunta: **Quais são as marcas de veículos mais propensas a receber multas por estacionamento irregular?** O mapeador extrai as marcas de veículos, enquanto o reducer soma as ocorrências e organiza os resultados, mostrando as marcas que mais receberam multas.\n",
    "\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 11. Aplicando o job MapReduce 4\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, usaremos o arquivo Python `CodeTicket4.py` para aplicar o **job MapReduce**. Para isso basta ir ao diretório do arquivo e no terminal digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "    python CodeTicket4.py hdfs:///tickets/PV_2016.csv -r hadoop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 11.1 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Job Completed Successfully**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado. No nosso exemplo, vemos a quantidade de filmes que receberam cada tipo de nota:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "\"FORD\"\t1324765\n",
    "\"TOYOT\"\t1154783\n",
    "\"HONDA\"\t1014073\n",
    "\"NISSA\"\t834832\n",
    "\"CHEVR\"\t759659\n",
    "\"FRUEH\"\t423583\n",
    "\"ME/BE\"\t362575\n",
    "\"DODGE\"\t359201\n",
    "\"BMW\"\t353302\n",
    "\"JEEP\"\t302510\n",
    "\"INTER\"\t285073\n",
    "\"GMC\"\t282249\n",
    "\"HYUND\"\t266087\n",
    "\"LEXUS\"\t238846\n",
    "\"ACURA\"\t193039\n",
    "\"CHRYS\"\t186158\n",
    "\"VOLKS\"\t183601\n",
    "\"INFIN\"\t158301\n",
    "\"NS/OT\"\t136078\n",
    "\"SUBAR\"\t130597\n",
    "\"ISUZU\"\t123733\n",
    "\"AUDI\"\t123702\n",
    "\"MITSU\"\t108494\n",
    "\"MAZDA\"\t104430\n",
    "\"LINCO\"\t103822\n",
    "\"HINO\"\t99459\n",
    "\"KIA\"\t98040\n",
    "\"CADIL\"\t85540\n",
    "\"MERCU\"\t77175\n",
    "\"VOLVO\"\t76708\n",
    "\"unknown\"\t63578\n",
    "\"ROVER\"\t60906\n",
    "\"WORKH\"\t44654\n",
    "\"KENWO\"\t43845\n",
    "\"BUICK\"\t39685\n",
    "\"PETER\"\t37505\n",
    "\"MACK\"\t26999\n",
    "\"PONTI\"\t24083\n",
    "\"MINI\"\t23533\n",
    "\"PORSC\"\t22454\n",
    "\"SATUR\"\t22168\n",
    "\"SMART\"\t18953\n",
    "\"JAGUA\"\t14317\n",
    "\"UD\"\t14107\n",
    "\"WORK\"\t13606\n",
    "\"FIAT\"\t13228\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: A marca **FORD** foi a mais multada, com **1.324.765 infrações**. As marcas **TOYOT** (Toyota) e **HONDA** também aparecem frequentemente na lista, com **1.154.783** e **1.014.073** infrações, respectivamente.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 12. <u>Criando Arquivo Python com o Job para o MapReduce 5</u>\n",
    "\n",
    "<br>\n",
    "\n",
    "> Quais são as ruas mais propensas a receber multas por estacionamento irregular?\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `CodeTicket5.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class InfracoesPorNomeRua(MRJob):\n",
    "\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_nome_rua_infracao,\n",
    "                   reducer=self.reducer_count_infracoes),\n",
    "            MRStep(reducer=self.reducer_top_nomes_ruas)\n",
    "        ]\n",
    "\n",
    "    # Mapper: Extrai o nome de rua de cada linha\n",
    "    def mapper_get_nome_rua_infracao(self, _, line):\n",
    "\n",
    "    \tfields = line.split(',')\n",
    "\n",
    "    \tif fields[0].isdigit():\n",
    "    \t\ttry:\n",
    "    \t\t\t# Extrair o nome da rua (Street Name)\n",
    "    \t\t\tnome_rua = fields[24].strip().upper() if fields[24].strip() else \"unknown\"\n",
    "\n",
    "    \t\t\t# Emitir a marca do veículo\n",
    "    \t\t\tyield nome_rua, 1\n",
    "\n",
    "    \t\texcept IndexError:\n",
    "    \t\t\treturn\n",
    "\n",
    "    # Reducer: Soma as ocorrências de cada nome de rua\n",
    "    def reducer_count_infracoes(self, key, values):\n",
    "    \tyield None, (sum(values), key)\n",
    "\n",
    "\n",
    "    # Reducer final: Seleciona os nomes de ruas com mais infrações\n",
    "    def reducer_top_nomes_ruas(self, _, nome_ruas):\n",
    "\n",
    "        # Ordena pela contagem em ordem decrescente e exibe todos os nomes de ruas\n",
    "        ruas_ordenadas = sorted(nome_ruas, reverse=True, key=lambda x: x[0])\n",
    "        \n",
    "        for count, nome in ruas_ordenadas:\n",
    "            # Exibe apenas ruas com 10000 ou mais infrações\n",
    "            if count >= 1000:\n",
    "                yield nome, count\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    InfracoesPorNomeRua.run()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 12.1 Explicando Código do Job MapReduce 5 (`CodeTicket5.py`)\n",
    "\n",
    "<br>\n",
    "\n",
    "O script `CodeTicket5` foi desenvolvido para identificar as ruas com o maior número de multas de estacionamento, utilizando o framework MapReduce para processamento de dados. Este código segue duas etapas principais: um mapeamento para extrair e contar as ocorrências de cada rua e uma redução para ordenar e selecionar as ruas com mais infrações.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Mapper:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Mapper: Extrai o nome de rua de cada linha\n",
    "    def mapper_get_nome_rua_infracao(self, _, line):\n",
    "\n",
    "    \tfields = line.split(',')\n",
    "\n",
    "    \tif fields[0].isdigit():\n",
    "    \t\ttry:\n",
    "    \t\t\t# Extrair o nome da rua (Street Name)\n",
    "    \t\t\tnome_rua = fields[24].strip().upper() if fields[24].strip() else \"unknown\"\n",
    "\n",
    "    \t\t\t# Emitir a marca do veículo\n",
    "    \t\t\tyield nome_rua, 1\n",
    "\n",
    "    \t\texcept IndexError:\n",
    "    \t\t\treturn\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `mapper_get_nome_rua_infracao`:\n",
    "\n",
    "- **Divisão da linha**: A função divide cada linha de entrada, utilizando a vírgula como delimitador (`line.split(',')`).\n",
    "- **Validação de campos**: A linha é validada para garantir que o campo `Summons Number` (posição 0) é numérico. Isso ajuda a evitar o cabeçalho e linhas inválidas.\n",
    "- **Extração do nome da rua**: Extrai o nome da rua da coluna na posição 24. Caso esteja vazio, o nome da rua é substituído por `\"unknown\"`.\n",
    "- **Emitir chave-valor**: A função emite o nome da rua como chave e `1` como valor, representando uma ocorrência de infração.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Reducer:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Reducer: Soma as ocorrências de cada nome de rua\n",
    "    def reducer_count_infracoes(self, key, values):\n",
    "    \tyield None, (sum(values), key)\n",
    "\n",
    "\n",
    "    # Reducer final: Seleciona os nomes de ruas com mais infrações\n",
    "    def reducer_top_nomes_ruas(self, _, nome_ruas):\n",
    "\n",
    "        # Ordena pela contagem em ordem decrescente e exibe todos os nomes de ruas\n",
    "        ruas_ordenadas = sorted(nome_ruas, reverse=True, key=lambda x: x[0])\n",
    "        \n",
    "        for count, nome in ruas_ordenadas:\n",
    "            # Exibe apenas ruas com 10000 ou mais infrações\n",
    "            if count >= 1000:\n",
    "                yield nome, count\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `reducer_count_infracoes`:\n",
    "\n",
    "- **Contagem das ocorrências**: Soma todas as ocorrências de cada rua, agrupando-as por nome.\n",
    "- **Saída**: Emite uma tupla com o total de infrações (soma) e o nome da rua, para cada rua identificada.\n",
    "\n",
    "<br>\n",
    "\n",
    "A função `reducer_top_nomes_ruas`:\n",
    "\n",
    "- **Ordenação e filtragem**: Ordena as ruas pelo número de infrações, de forma decrescente. Em seguida, exibe apenas as ruas com pelo menos 1.000 infrações.\n",
    "- **Saída**: Emite o nome da rua e o número de infrações, mostrando apenas as ruas com contagem mínima de infrações conforme o critério especificado.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 12.2 Resumo\n",
    "\n",
    "- O `CodeTicket5.py` utiliza a metodologia MapReduce para processar o conjunto de dados de infrações de estacionamento e identificar as ruas com maior incidência de multas. O mapeador extrai e conta as infrações por rua, enquanto o reducer organiza e filtra os resultados para exibir apenas as ruas com pelo menos 1.000 infrações, atendendo ao critério de exibição.\n",
    "\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 13. Aplicando o job MapReduce 5\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, usaremos o arquivo Python `CodeTicket5.py` para aplicar o **job MapReduce**. Para isso basta ir ao diretório do arquivo e no terminal digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "    python CodeTicket5.py hdfs:///tickets/PV_2016.csv -r hadoop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 13.1 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Job Completed Successfully**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado. No nosso exemplo, vemos a quantidade de filmes que receberam cada tipo de nota:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "\"BROADWAY\"\t229790\n",
    "\"3RD AVE\"\t173303\n",
    "\"5TH AVE\"\t113589\n",
    "\"MADISON AVE\"\t107016\n",
    "\"LEXINGTON AVE\"\t89513\n",
    "\"2ND AVE\"\t83166\n",
    "\"1ST AVE\"\t70951\n",
    "\"7TH AVE\"\t70265\n",
    "\"8TH AVE\"\t65123\n",
    "\"AMSTERDAM AVE\"\t63560\n",
    "\"QUEENS BLVD\"\t60794\n",
    "\"6TH AVE\"\t58061\n",
    "\"JAMAICA AVE\"\t48297\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: As ruas **BROADWAY**, **3RD AVE**, e **5TH AVE** destacam-se como as vias mais propensas a receber multas de estacionamento irregular, com contagens de **229.790**, **173.303** e **113.589** infrações, respectivamente. Essas ruas representam áreas com alta concentração de infrações, sinalizando pontos críticos para intervenções regulatórias e de fiscalização. \n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 14. <u>Criando Arquivo Python com o Job para o MapReduce 6</u>\n",
    "\n",
    "<br>\n",
    "\n",
    "> Quais são códigos de rua mais propensos a receber multas por estacionamento irregular?\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `CodeTicket6.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class InfracoesPorCodigoRua(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_street_codes,\n",
    "                   reducer=self.reducer_count_codes),\n",
    "            MRStep(reducer=self.reducer_sort_top_codes)\n",
    "        ]\n",
    "\n",
    "    # Mapper: Extrai cada código de rua presente nas três colunas\n",
    "    def mapper_get_street_codes(self, _, line):\n",
    "        fields = line.split(',')\n",
    "        \n",
    "        if fields[0].isdigit():\n",
    "            try:\n",
    "                # Extrair os códigos de rua (Street Code1, Street Code2, Street Code3)\n",
    "                street_codes = [fields[9].strip(), fields[10].strip(), fields[11].strip()]\n",
    "                \n",
    "                # Emitir cada código de rua válido como uma chave\n",
    "                for code in street_codes:\n",
    "                    if code:  # Ignorar códigos vazios\n",
    "                        yield code, 1\n",
    "\n",
    "            except IndexError:\n",
    "                return  # Ignorar linhas com índice inválido\n",
    "\n",
    "    # Reducer: Soma as ocorrências de cada código de rua\n",
    "    def reducer_count_codes(self, code, counts):\n",
    "        yield None, (sum(counts), code)\n",
    "\n",
    "    # Reducer final: Ordena os códigos de rua por contagem de ocorrências\n",
    "    def reducer_sort_top_codes(self, _, code_counts):\n",
    "        # Ordena pela contagem em ordem decrescente\n",
    "        sorted_codes = sorted(code_counts, reverse=True, key=lambda x: x[0])\n",
    "        \n",
    "        for count, code in sorted_codes:\n",
    "            # Exibe apenas códigos com 10000 ou mais infrações\n",
    "            if count >= 1000:\n",
    "                yield code, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    InfracoesPorCodigoRua.run()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 14.1 Explicando Código do Job MapReduce 6 (`CodeTicket6.py`)\n",
    "\n",
    "<br>\n",
    "\n",
    "O script `CodeTicket6` foi desenvolvido para identificar os códigos de rua que aparecem com maior frequência em infrações de estacionamento, indicando as áreas mais propensas a multas. Este código usa o framework MapReduce e segue duas etapas principais: mapeamento e redução, conforme detalhado a seguir.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Mapper:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    def mapper_get_street_codes(self, _, line):\n",
    "        fields = line.split(',')\n",
    "        \n",
    "        if fields[0].isdigit():\n",
    "            try:\n",
    "                # Extrair os códigos de rua (Street Code1, Street Code2, Street Code3)\n",
    "                street_codes = [fields[9].strip(), fields[10].strip(), fields[11].strip()]\n",
    "                \n",
    "                # Emitir cada código de rua válido como uma chave\n",
    "                for code in street_codes:\n",
    "                    if code:  # Ignorar códigos vazios\n",
    "                        yield code, 1\n",
    "\n",
    "            except IndexError:\n",
    "                return  # Ignorar linhas com índice inválido\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `mapper_get_street_codes`:\n",
    "\n",
    "- **Divisão da linha**: Divide a linha de entrada usando vírgulas como delimitador para obter cada campo individual (`line.split(',')`).\n",
    "- **Validação de campos**: A função verifica se o campo `Summons Number` (posição 0) é numérico, filtrando o cabeçalho ou linhas inválidas.\n",
    "- **Extração dos códigos de rua**: Extrai os códigos das colunas `Street Code1`, `Street Code2` e `Street Code3` (posições 9, 10 e 11).\n",
    "- **Emitir chave-valor**: Emite cada código de rua válido como uma chave e `1` como valor, representando uma ocorrência de infração.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Reducer:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Reducer: Soma as ocorrências de cada código de rua\n",
    "    def reducer_count_codes(self, code, counts):\n",
    "        yield None, (sum(counts), code)\n",
    "\n",
    "    # Reducer final: Ordena os códigos de rua por contagem de ocorrências\n",
    "    def reducer_sort_top_codes(self, _, code_counts):\n",
    "        # Ordena pela contagem em ordem decrescente\n",
    "        sorted_codes = sorted(code_counts, reverse=True, key=lambda x: x[0])\n",
    "        \n",
    "        for count, code in sorted_codes:\n",
    "            # Exibe apenas códigos com 10000 ou mais infrações\n",
    "            if count >= 1000:\n",
    "                yield code, count\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A função `reducer_count_codes`:\n",
    "\n",
    "- **Contagem das ocorrências**: Soma as ocorrências de cada código de rua individualmente.\n",
    "- **:Saída**:: Emite uma tupla com o total de ocorrências e o código de rua para cada código identificado.\n",
    "\n",
    "<br>\n",
    "\n",
    "A função `reducer_sort_top_codes`:\n",
    "\n",
    "- **Ordenação e filtragem**: Ordena os códigos de rua pela contagem de ocorrências em ordem decrescente. Após a ordenação, filtra os resultados para mostrar apenas os códigos com um mínimo de 1.000 ocorrências.\n",
    "- **Saída**: Emite o código de rua e o número total de infrações, exibindo apenas os códigos mais frequentes.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 14.2 Resumo\n",
    "\n",
    "- Este script utiliza MapReduce para processar os dados de infrações de estacionamento e identificar os códigos de rua mais recorrentes em multas. O mapeador extrai e conta as infrações por código de rua, enquanto o reducer organiza e exibe os resultados para destacar os códigos de rua com o maior número de infrações, considerando apenas aqueles que aparecem com frequência mínima de 1.000 infrações.\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 15. Aplicando o job MapReduce 6\n",
    "\n",
    "<br>\n",
    "\n",
    "No contexto do nosso laboratório, usaremos o arquivo Python `CodeTicket6.py` para aplicar o **job MapReduce**. Para isso basta ir ao diretório do arquivo e no terminal digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "    python CodeTicket6.py hdfs:///tickets/PV_2016.csv -r hadoop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 15.1 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Job Completed Successfully**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado. No nosso exemplo, vemos a quantidade de filmes que receberam cada tipo de nota:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "\"0\"\t8504821\n",
    "\"40404\"\t624805\n",
    "\"13610\"\t408288\n",
    "\"10210\"\t315060\n",
    "\"10410\"\t290852\n",
    "\"10510\"\t263155\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: O código de rua `\"0\"` foi o mais frequente, com **8.504.821 infrações**, seguido por outros códigos como `\"40404\"` e `\"13610\"`, com **624.805** e **408.288** infrações, respectivamente. Esses códigos indicam áreas específicas que têm uma alta concentração de infrações de estacionamento, sendo potenciais focos para uma fiscalização mais intensa e para revisões de regulamentação.\n",
    "\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br><br><br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986907ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7412410c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88315d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8512413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2a9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
