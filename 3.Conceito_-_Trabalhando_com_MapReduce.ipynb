{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe735a8f",
   "metadata": {},
   "source": [
    "# <center>Lab Trabalhando com o MapReduce</center>\n",
    "\n",
    "### IMPORTANTE\n",
    "\n",
    "- Para este laboratório foram fornecidos os arquivos `AvaliaFilme.py` e `AnalisaFilme.py` contendo **Jobs de MapReduce** utilizando a linguagem Python e também o arquivo `ml-100k.zip` contendo o **dataset** a ser usado.\n",
    "\n",
    "<br> <b>\n",
    "    \n",
    "---\n",
    "    \n",
    "    \n",
    "<br> <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9f947",
   "metadata": {},
   "source": [
    "# O que é MapReduce?\n",
    "\n",
    "O MapReduce é um modelo de programação distribuído utilizado principalmente em frameworks como o Hadoop para processar grandes volumes de dados. Ele permite que problemas complexos sejam divididos em pequenas partes que podem ser processadas paralelamente em um cluster de servidores. Esse modelo é particularmente útil para cenários de Big Data, onde o volume de dados é grande demais para ser processado de maneira eficiente em uma única máquina.\n",
    "\n",
    "### Como Funciona o Modelo MapReduce\n",
    "\n",
    "O modelo MapReduce é composto por duas fases principais: **Map** (Mapeamento) e **Reduce** (Redução), além de uma fase intermediária chamada **Shuffle**.\n",
    "\n",
    "- **1. Fase de Mapeamento (Map)**: O processo começa com o cientista de dados analisando o problema e definindo como os dados de entrada serão representados em pares de chave e valor. Por exemplo, em uma contagem de palavras em um conjunto de textos, as palavras individuais podem ser as chaves e a contagem (geralmente 1) seria o valor associado.\n",
    "\n",
    "O cientista de dados desenvolve um programa de mapeamento (Mapper), que aplica essas regras de negócios, \"quebrando\" os dados de entrada em pequenos pares de chave/valor. Cada bloco de dados de entrada é então processado pelo Mapper, que gera os pares conforme definido no código.\n",
    "\n",
    "**Exemplo**: Se os dados de entrada forem \"Deer, Bear, River, Car\", o Mapper pode gerar os seguintes pares: (\"Deer\", 1), (\"Bear\", 1), (\"River\", 1), (\"Car\", 1).\n",
    "\n",
    "<br>\n",
    "\n",
    "- **2. Fase Intermediária** - Shuffle: Após a fase de mapeamento, ocorre a fase de Shuffle. O Shuffle é executado automaticamente pelo framework (neste caso, o Hadoop), sem que o cientista de dados precise intervir diretamente. A função dessa fase é agrupar todos os pares de chave/valor gerados pela etapa de mapeamento.\n",
    "\n",
    "**Exemplo**: Se em várias partes do texto foram encontradas várias ocorrências da palavra \"Car\", o Shuffle agrupará todas essas ocorrências em um único lugar para que possam ser processadas na fase seguinte.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **3. Fase de Redução (Reduce)**: Na fase de Reduce, os pares agrupados pelo Shuffle são processados para gerar o resultado final. O cientista de dados define como será a redução dos dados. No caso da contagem de palavras, a redução pode ser simplesmente somar os valores para cada chave.\n",
    "\n",
    "**Exemplo**: Se os pares de chave/valor após o Shuffle são (\"Car\", [1, 1, 1]), a fase de redução somará esses valores, resultando em (\"Car\", 3).\n",
    "\n",
    "Essa fase de redução retorna a informação processada e consolidada que o cientista de dados precisa para resolver o problema em questão. No exemplo, a saída seria uma lista das palavras e suas respectivas contagens, como (\"Car\", 3), (\"Deer\", 2), (\"Bear\", 2).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3adfb",
   "metadata": {},
   "source": [
    "# Objetivo do Laboratório\n",
    "\n",
    "<br>\n",
    "\n",
    "Utilizando um **dataset de sistema de recomendação de filmes**, o objetivo é **executar um programa** que conte a quantidade de avaliações recebidas para cada tipo de nota atribuída aos filmes.\n",
    "\n",
    "Em outras palavras, vamos verificar **quantas vezes cada filme recebeu determinadas avaliações**.\n",
    "\n",
    "Por exemplo, se um filme recebeu a nota <i>**4 estrelas**</i> uma vez ou <i>**5 estrelas**</i> sete vezes, isso será contabilizado e apresentado de forma estruturada.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Pergunta de Negócio\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Quantas avaliações de cada tipo um filme recebeu?**\n",
    "\n",
    "Esta questão nos ajudará a entender o comportamento das avaliações, identificando quais tipos de notas são mais comuns e qual é a distribuição das avaliações para diferentes filmes.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82452e5",
   "metadata": {},
   "source": [
    "# Sobre o Dataset\n",
    "\n",
    "<br>\n",
    "\n",
    "Para este laboratório usaremos o dataset **MovieLens 100K** que é um dos conjuntos de dados mais utilizados para sistemas de recomendação e foi criado pelo GroupLens Research. Ele é frequentemente empregado para avaliar o desempenho de algoritmos colaborativos e análise preditiva na recomendação de filmes.\n",
    "\n",
    "---\n",
    "\n",
    "### Descrição Geral do Dataset MovieLens 100K\n",
    "\n",
    "- **Lançamento**: Abril de 1998\n",
    "- **Tamanho**: 100.000 avaliações\n",
    "- **Número de usuários**: 1.000\n",
    "  - Cada usuário avaliou pelo menos 20 filmes.\n",
    "- **Número de filmes**: 1.700\n",
    "  - Filmes lançados até 1998, com múltiplos gêneros.\n",
    "- **Formato das avaliações**: Escala de 1 a 5 estrelas.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Estrutura dos Arquivos\n",
    "\n",
    "- **`u.data`** (**será usado neste laboratório**):\n",
    "  - Contém as avaliações no formato (usuário, filme, nota, timestamp).\n",
    "  - **Exemplo**: 196 242 3 881250949 (Usuário 196 deu nota 3 para o filme 242 no timestamp correspondente).\n",
    "\n",
    "- **`u.user`**: \n",
    "  - Informações sobre os usuários, como (ID, idade, sexo, ocupação, CEP).\n",
    "  - **Exemplo**: 1|24|M|technician|85711.\n",
    "\n",
    "- **`u.item`**:\n",
    "  - Detalhes dos filmes, como (ID, título, data de lançamento, gêneros).\n",
    "  - **Exemplo**: 1|Toy Story (1995)|01-Jan-1995|Animation|Children's|Comedy.\n",
    "\n",
    "- **`u.genre`**:\n",
    "  - Lista os gêneros disponíveis, como Action, Comedy, Drama, etc.\n",
    "\n",
    "- **`u.occupation`**:\n",
    "  - Lista as ocupações dos usuários.\n",
    "  \n",
    "<br>\n",
    "\n",
    "- **Arquivos de Treino e Teste** (`*.base` e `*.test`):\n",
    "\n",
    " - O dataset é dividido em conjuntos para **treinamento** e **teste**, usados para validar algoritmos de recomendação.\n",
    "  \n",
    "<br>\n",
    "\n",
    "- **README.txt**:\n",
    "  - Contém instruções sobre a utilização do dataset, incluindo os termos de licença.\n",
    "\n",
    "---\n",
    "\n",
    "### Aplicações e Casos de Uso\n",
    "\n",
    "- **Teste de Sistemas de Recomendação**: Avaliação de algoritmos colaborativos, como Filtragem Colaborativa e Modelos de Fatoração de Matrizes.\n",
    "- **Análise de Comportamento do Usuário**: Identificação de padrões nas avaliações e preferências.\n",
    "- **Treinamento e Validação de Modelos de Machine Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac517c",
   "metadata": {},
   "source": [
    "## <br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# <center><u>Iniciando o Laboratório</u></center>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "## 1. Iniciando os Serviços\n",
    "\n",
    "<br>\n",
    "\n",
    "1.1 **Iniciar o HDFS (NameNode, DataNode, SecondaryNameNode)**:\n",
    "   ```bash\n",
    "   start-dfs.sh  |  stop-dfs.sh\n",
    "   ```\n",
    "1.2 **Iniciar o YARN (ResourceManager, NodeManager)**:\n",
    "   ```bash\n",
    "   start-yarn.sh  |  stop-yarn.sh\n",
    "   ```\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 2. Criando Pasta/Diretório com o nome de `mapred` para o Laboratório no HDFS\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    hdfs dfs -mkdir /mapred\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "2.1 **Lista os arquivos e diretórios no HDFS raiz**.\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "   hdfs dfs -ls /\n",
    "```\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 3. Copiando o Dataset do `Sistema Operacional Local` para dentro do `HDFS`:\n",
    "\n",
    "<br>\n",
    "\n",
    "**Para este laboratório precisaremos do arquivo `u.data` que está dentro da pasta extraída do dataset**.\n",
    "\n",
    "Abrir pasta onde está o arquivo a ser copiado, abrir um terminal e digitar o comando abaixo:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    hdfs dfs -put u.data /mapred\n",
    "```\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 4. Criando Arquivo Python com o Job para o MapReduce\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Código do Arquivo `AvaliaFilme.py`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRAvaliaFilme(MRJob):\n",
    "    def mapper(self, key, line):\n",
    "        # Dividimos cada linha por tabulação e extraímos os valores\n",
    "        (userID, movieID, rating, timestamp) = line.split('\\t')\n",
    "        # Emitimos a nota como chave e 1 como valor\n",
    "        yield rating, 1\n",
    "\n",
    "    def reducer(self, rating, occurences):\n",
    "        # Somamos todas as ocorrências da mesma avaliação\n",
    "        yield rating, sum(occurences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Executa o job MapReduce\n",
    "    MRAvaliaFilme.run()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.1 Entendendo as 3 Fases do Processo de <i>MapReduce</i> considerando o Script `AvaliaFilme.py`\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### Fase 1 – Mapeamento (Mapper)\n",
    "\n",
    "![Example Image](Analytics/image1.png)\n",
    "\n",
    "Na fase de mapeamento, o objetivo é **contar quantas vezes cada filme foi avaliado com uma determinada nota**. Utilizamos a palavra reservada `yield` para definir a **chave** que será usada no processo. No nosso caso, a **coluna** `rating` é escolhida como chave, pois queremos saber o **número total de filmes avaliados para cada tipo de nota**, variando de **1 a 5 estrelas**.\n",
    "\n",
    "Durante esta fase, cada **rating** é **mapeado** e associado ao valor **1**, que representa uma única ocorrência dessa nota para um filme. Este passo é crucial para contabilizar a frequência de cada avaliação.\n",
    "\n",
    "O **código** responsável por essa fase é escrito pelo **Cientista de Dados**, que define a lógica de como processar as linhas de entrada para gerar os pares de **chave-valor**.\n",
    "\n",
    "#### Explicando o trecho do código:\n",
    "\n",
    "- A função `mapper` recebe como entrada cada linha do arquivo de dados.\n",
    "- A linha é dividida em **quatro partes**: `userID`, `movieID`, `rating` e `timestamp`, com os valores separados por tabulações (`\\t`).\n",
    "- A chave escolhida é a **nota (rating)** dada ao filme, enquanto o valor é o número **1**. Isso indica que para cada avaliação feita, estamos registrando **uma ocorrência** da nota.\n",
    "- O `yield` é a palavra-chave que emite cada par **chave-valor** para a próxima fase do processo.\n",
    "\n",
    "<br>\n",
    "\n",
    "Esse processo de mapeamento é fundamental para preparar os dados para a próxima fase, onde as avaliações serão agrupadas e somadas.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### Fase 2 – Shuffle e Sort\n",
    "\n",
    "Essa fase é **processada automaticamente pelo framework MapReduce**. Não há um código explícito para essa fase no script, pois o próprio framework cuida de:\n",
    "\n",
    "- Agrupar todos os pares de **chave-valor** emitidos pela função `mapper`.\n",
    "- Organizar os dados com base nas **chaves iguais** (neste caso, os ratings de 1 a 5).\n",
    "- Após o agrupamento, os pares chave-valor para as notas são semelhantes ao seguinte exemplo:\n",
    "\n",
    "**Exemplo de Agrupamento:**\n",
    "\n",
    "```makefile\n",
    "    1: [1, 1]\n",
    "    2: [1, 1, 1]\n",
    "    3: [1, 1]\n",
    "    4: [1]\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### Fase 3 – Redução (Reducer)\n",
    "\n",
    "![Example Image](Analytics/image2.png)\n",
    "\n",
    "Na fase de **redução**, o código aplica um cálculo matemático para somar as ocorrências de cada nota (rating). Utilizamos a função `sum()` para agregar todas as ocorrências de uma nota específica, resultando na **quantidade total de filmes avaliados** com aquela nota.\n",
    "\n",
    "Essa fase é fundamental, pois transforma os dados mapeados em informações úteis: quantos filmes receberam cada tipo de avaliação.\n",
    "\n",
    "O **Cientista de Dados** é responsável por definir como essa redução será aplicada, usando a lógica apropriada.\n",
    "\n",
    "#### Explicando o trecho do código:\n",
    "\n",
    "- A função `reducer` recebe como entrada a **nota (rating)** como chave, e uma lista de **ocorrências** (que são os números 1 gerados na fase de mapeamento).\n",
    "- A função `sum()` é aplicada a essa lista, somando todas as ocorrências de cada nota.\n",
    "- O `yield` emite o **rating (chave)** e o total de ocorrências (quantidade de filmes que receberam aquela nota).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Exemplo de Resultado Final**:\n",
    "\n",
    "```makefile\n",
    "    1: 2\n",
    "    2: 3\n",
    "    3: 2\n",
    "    4: 1\n",
    "```\n",
    "\n",
    "**Isso indica que**: 2 filmes receberam nota 1, 3 filmes receberam nota 2, 2 filmes receberam nota 3 e 1 filme recebeu nota 4\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "### Resumo das 3 Fases com Código Explicado\n",
    "\n",
    "- **Mapper (Fase 1)**: Divide as linhas do arquivo e emite pares de chave-valor, onde a **chave é a nota** (`rating`) e o **valor é 1** para cada ocorrência da nota.\n",
    "- **Shuffle e Sort (Fase 2)**: O framework organiza automaticamente as notas iguais e agrupa as ocorrências.\n",
    "- **Reducer (Fase 3)**: Soma as ocorrências de cada nota, retornando o total de filmes para cada tipo de nota.\n",
    "\n",
    "<br>\n",
    "\n",
    "Esse processo permite **contabilizar o número de filmes** que receberam cada tipo de avaliação (de 1 a 5 estrelas) no dataset.\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n",
    "\n",
    "## 5. Executando Arquivo Python com o Job para o MapReduce\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Como Executar um Job MapReduce ?\n",
    "\n",
    "<br>\n",
    "\n",
    "> Para executar um **Job MapReduce**, precisamos rodar um script Python que utiliza o framework Hadoop para processar os dados. Aqui está uma **explicação sobre como realizar essa execução**.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "#### 5.1 Executando o Script Python:\n",
    "\n",
    "\n",
    "Normalmente, para executar um script Python, bastaria abrir um terminal no diretório onde o arquivo se encontra e digitar:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    python AvaliaFilme.py\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 5.2 Passando o Conjunto de Dados como Entrada:\n",
    "\n",
    "No contexto do nosso laboratório, o arquivo Python `AvaliaFilme.py` **precisa receber como entrada um conjunto de dados que está armazenado no HDFS (sistema de arquivos distribuído do Hadoop)**.\n",
    "\n",
    "Para isso, precisamos indicar corretamente o caminho do arquivo de dados no **HDFS** durante a execução do script, da seguinte forma:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    python AvaliaFilme.py hdfs:///mapred/u.data -r hadoop\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- `hdfs:///mapred/u.data` é o caminho do arquivo de dados no HDFS.\n",
    "- `-r hadoop` indica que o job será executado usando o Hadoop como backend.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 5.3 Explicando a Saída no Terminal (Indicadores de Execução Bem-Sucedida)\n",
    "\n",
    "<i>**Job Completado com Sucesso**</i>: Quando o job é finalizado sem erros, você verá uma linha de confirmação, indicando que o processamento foi concluído com sucesso:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "    Job job_1728940457090_0001 completed successfully\n",
    "```\n",
    "<br>\n",
    "\n",
    "<i>**Map e Reduce Concluídos**</i>: O processo de MapReduce envolve duas etapas: **mapeamento (map)** e **redução (reduce)**. O status de 100% para ambas as fases confirma que o processamento foi concluído corretamente:\n",
    "\n",
    "```bash\n",
    "    map 100% reduce 100%\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Resultado Final Exibido**</i>: Após a execução bem-sucedida do job, o resultado é extraído do **HDFS** e apresentado. No nosso exemplo, vemos a quantidade de filmes que receberam cada tipo de nota:\n",
    "\n",
    "```bash\n",
    "    \"1\"  6110\n",
    "    \"2\"  11370\n",
    "    \"3\"  27145\n",
    "    \"4\"  34174\n",
    "    \"5\"  21201\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isso significa que**: 6.110 filmes receberam nota 1, 11.370 filmes receberam nota 2, 27.145 filmes receberam nota 3, 34.174 filmes receberam nota 4 e 21.201 filmes receberam nota 5.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>**Limpeza dos Diretórios Temporários**</i>: Após a conclusão do job, os diretórios temporários no **HDFS** são removidos automaticamente para manter o sistema organizado. Isso garante que nenhum arquivo desnecessário permaneça no sistema após o processamento:\n",
    "\n",
    "```bash\n",
    "    Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/AvaliaFilme.hadoop.20241016.205700.452291...\n",
    "```\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1092fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b2ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc851cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986907ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
